{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34",
   "metadata": {
    "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 5:** Working with Large Documents</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the previous notebook, we learned about running state chains and knowledge bases! By the end, we had all the tools necessary to do some simple dialog management and custom knowledge tracking. In this notebook, we will take the same ideas and move towards the space of large documents, considering what kinds of issues we will run into as we try to incorporate large files into our LLM contexts.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Get familiar with document loaders and the kinds of utilities they might provide you.\n",
    "- Learn how to parse large documents with limited context room by chunking the document and building up a knowledge base progressively.\n",
    "- Understand how the progressive recontextualization, coersion, and consolidation of document chunks can be extremely useful, and also where it will encounter natural limitations.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- Looking at the chunks that come out of your ArxivParser, you'll notice that some of the chunks make little sense on their own or have been completely corrupted by the conversion to text. Is it doing a pass over the chunks to clean them up?\n",
    "- Considering the document summarization workflow (or any similar workflow that processes through a large list of document chunks), how often should this happen, and when is it justifiable?\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52",
   "metadata": {
    "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -qq langchain langchain-nvidia-ai-endpoints gradio\n",
    "# %pip install -qq arxiv pymupdf\n",
    "\n",
    "# import os\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-...\"\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c33c07-19b8-4c81-8d99-30fa2b3b2017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:237: UserWarning: Default model is set as: 01-ai/yi-large. \n",
      "Set model using model parameter. \n",
      "To get available models use available_models property.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='abacusai/dracarys-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ai21labs/jamba-1.5-large-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ai21labs/jamba-1.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='baichuan-inc/baichuan2-13b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='databricks/dbrx-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-dbrx-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-deepseek-coder-6_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-0528', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-llama-8b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-14b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-32b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-7b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/codegemma-1.1-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-1.1-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-2b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3-12b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3-1b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3-27b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3-4b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3n-e2b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3n-e4b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/recurrentgemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-recurrentgemma-2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/shieldgemma-9b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='gotocompany/gemma-2-9b-cpt-sahabatai-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-3.0-3b-a800m-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-3.0-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-3.3-8b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-34b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-34b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-8b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-8b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-guardian-3.0-8b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='igenius/colosseum_355b_instruct_16k', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='igenius/italia_10b_instruct_16k', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='marin/marin-8b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/codellama-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codellama-70b', 'playground_llama2_code_70b', 'llama2_code_70b', 'playground_llama2_code_34b', 'llama2_code_34b', 'playground_llama2_code_13b', 'llama2_code_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-11b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-1b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-90b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-90b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-4-maverick-17b-128e-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-4-scout-17b-16e-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-guard-4-12b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-moe-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-4-mini-flash-reasoning', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-4-mini-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-4-multimodal-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='mistralai/magistral-small-2506', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mathstral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large-2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='mistralai/mistral-medium-3-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-nemotron', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-small-24b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-small-3.1-24b-instruct-2503', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='moonshotai/kimi-k2-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/embed-qa-4', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemoguard-8b-content-safety', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemoguard-8b-topic-control', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-51b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-70b-reward', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-4b-v1.1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-8b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-vl-8b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-ultra-253b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.2-nemoretriever-300m-embed-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.2-nv-embedqa-1b-v1', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.2-nv-embedqa-1b-v2', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.3-nemotron-super-49b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.3-nemotron-super-49b-v1.5', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-70b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/mistral-nemo-minitron-8b-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/mistral-nemo-minitron-8b-base', model_type='completions', client='NVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nemoretriever-parse', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-340b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['qa-nemotron-4-340b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-mini-hindi-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/nemotron-mini-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/neva-22b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b', aliases=['ai-neva-22b', 'playground_neva_22b', 'neva_22b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embed-v1', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=['ai-nv-embed-v1'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embedcode-7b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embedqa-e5-v5', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embedqa-mistral-7b-v2', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nvclip', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/riva-translate-4b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/usdcode-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/vila', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/vila', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='openai/gpt-oss-120b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='openai/gpt-oss-20b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='opengpt-x/teuken-7b-instruct-commercial-v0.4', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-7b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-32b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen3-235b-a22b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwq-32b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='sarvamai/sarvam-m', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='snowflake/arctic-embed-l', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=['ai-arctic-embed-l'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='speakleash/bielik-11b-v2.3-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='thudm/chatglm3-6b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='tiiuae/falcon3-7b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='tokyotech-llm/llama-3-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='utter-project/eurollm-9b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-creative-122b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-fin-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b-32k'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='zyphra/zamba2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8583a1-c10a-41da-8256-49520f868670",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful utility method for printing intermediate states\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "\n",
    "def RPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def PPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        pprint(preface, x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Chatting with Documents\n",
    "\n",
    "This notebook will begin a longer stream of discussion surrounding the use of LLMs to chat with documents. In a world where chat models are trained on giant repositories of public data and retraining them on custom data is prohibitively expensive, the idea of having an LLM reason about a set of PDFs or even a YouTube video opens up many opportunities!\n",
    "\n",
    "- **Your LLM can have a modifiable knowledge base grounded in human-readible documents,** meaning that you can directly control what kinds of data it has access to and can instruct it to interact with it.\n",
    "\n",
    "- **Your LLM can sort through and pull references directly from your document set.** With sufficient prompt engineering and instruction-following priors, you can force your models to only act based on the material you provide.\n",
    "\n",
    "- **Your LLM can possibly even interact with your documents, making automatic modifications as necessary.** This opens up avenues in automatic content refinement and synthetic operations which will be explored later.\n",
    "\n",
    "Listing out some possibilities is pretty easy, and from there you can let your imagination run wild... but we haven't gained the tools to do this quite yet, right?\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Naive Approach: Stuff Your Document**\n",
    "\n",
    "Suppose you have some text documents (PDF, blog, etc.) and want to ask questions related to the contents of those documents. One approach you could try involves taking a representation of the document and feeding it all to a chat model! From a document perspective, this is known as [**document stuffing**](https://js.langchain.com/v0.1/docs/modules/chains/document/stuff/).\n",
    "\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=14DRI_uDviqzqg14TKoIc8IlBc3Zsb8oO\" width=800px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_stuff.png\" width=800px/>\n",
    ">\n",
    "> From [**Stuff | LangChain**ü¶úÔ∏èüîó](https://js.langchain.com/v0.1/docs/modules/chains/document/stuff/)\n",
    "\n",
    "<br>\n",
    "\n",
    "This may very well work if your model is strong enough and if your document is short enough, but it shouldn't be expected to work well for an entire document. Many modern LLMs have significant trouble working with long contexts due to training limitations. Nowadays large model deterioration isn't quite as catastrophic, but good instruction following is likely to fall apart pretty quickly regardless of which model you use (assuming you're accessing the raw model).\n",
    "\n",
    "<br>\n",
    "\n",
    "**The key issues you'll need to resolve with document reasoning are:**\n",
    "\n",
    "- How do we split our documents into pieces that can be reasoned with?\n",
    "\n",
    "- How can we find and consider these pieces efficiently as the size and number of documents increases?\n",
    "\n",
    "This course will explore several approaches to address these issues while continuing to build up our LLM orchestration skills. ***This notebook will serve to expand out our previous running chain skills for more progressive reasoning formulations, whereas the next notebooks will introduce some new techniques to properly address retrieval at scale.*** Through this experience, we will continue to leverage cutting-edge open-source solutions to make our solutions standard and integratable.\n",
    "\n",
    "Speaking of, the field of document loading frameworks has many strong options, and two major players will come up throughout the course:\n",
    "\n",
    "- [**LangChain**](https://python.langchain.com/docs/get_started/introduction) provides a simple framework for connecting LLMs to your own data sources via general chunking strategies and strong incorporation with embedding frameworks/services. This framework has initially grown around its strong general support for LLM features, which signals its active strengths closer to the chain abstractions and agent coordination.\n",
    "\n",
    "- [**LlamaIndex**](https://gpt-index.readthedocs.io/en/stable/) is a data framework for LLM applications to ingest, structure, and access private or domain-specific data. It has since branched out to include general LLM capabilities similar to LangChain, but as of now it is still strongest in addressing the document side of LLM components since its initial abstractions were centered around that problem.\n",
    "\n",
    "It's recommended to read more about the unique strengths of both LlamaIndex and LangChain and pick the one that works best for you. Since LlamaIndex can be used *with* LangChain, the frameworks' unique capabilities can be leveraged together without too much issue. For the sake of simplicity, we will stick to LangChain in this course and will allow the [**NVIDIA/GenerativeAIExamples repository**](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG/notebooks) to explore deeper LlamaIndex options for those interested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310462b-f215-4d00-9d59-e613921bed0a",
   "metadata": {
    "id": "3310462b-f215-4d00-9d59-e613921bed0a"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** Loading Documents\n",
    "\n",
    "LangChain provides a variety of [document loaders](https://python.langchain.com/docs/integrations/document_loaders) to facilitate the injestion of various document formats (HTML, PDF, code) from many different sources and locations (local storage, private s3 buckets, public websites, messaging APIs, etc.). These loaders query your data sources and return a `Document` object which contains the content and metadata, usually in a plain-text or otherwise human-readible format. There are plenty of document loaders already built and ready to use, with the first-party LangChain options listed [here](https://python.langchain.com/docs/integrations/document_loaders).\n",
    "\n",
    "**In this example, we can load a research paper of our choice using one of the following LangChain loaders:**\n",
    "- [`UnstructuredFileLoader`](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file): Generally-useful file loader for arbitrary files; doesn't make too many assumptions about your document structure and is usually sufficient.\n",
    "- [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv): A more specialized file-loader which can communicate with the Arxiv interface directly. [Just one example of many](https://python.langchain.com/docs/integrations/document_loaders), this will make some more assumptions about your data to yield nicer parsings and auto-fill metadata (useful when you have multiple documents/formats).\n",
    "\n",
    "For our code example we will default to using `ArxivLoader` to load in one of either the [MRKL](https://arxiv.org/abs/2205.00445) or [ReAct](https://arxiv.org/abs/2210.03629) publication papers as you're likely to run into them at some point in your continued chat model research endeavors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4382b61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3944,
     "status": "ok",
     "timestamp": 1703112979370,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "b4382b61",
    "outputId": "d6e95b9b-97be-4984-a9fd-58a528091146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 472 ms, sys: 86.1 ms, total: 558 ms\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "## Loading in the file\n",
    "\n",
    "## Unstructured File Loader: Good for arbitrary \"probably good enough\" loader\n",
    "# documents = UnstructuredFileLoader(\"llama2_paper.pdf\").load()\n",
    "\n",
    "## More specialized loader, won't work for everything, but simple API and usually better results\n",
    "documents = ArxivLoader(query=\"2404.16130\").load()  ## GraphRAG\n",
    "# documents = ArxivLoader(query=\"2404.03622\").load()  ## Visualization-of-Thought\n",
    "# documents = ArxivLoader(query=\"2404.19756\").load()  ## KAN: Kolmogorov-Arnold Networks\n",
    "# documents = ArxivLoader(query=\"2404.07143\").load()  ## Infini-Attention\n",
    "# documents = ArxivLoader(query=\"2210.03629\").load()  ## ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hw0SL--6cirp",
   "metadata": {
    "id": "hw0SL--6cirp"
   },
   "source": [
    "<br>\n",
    "\n",
    "We can see from our import that we this connector gives us access to two different components:\n",
    "- The `page_content` is the actual body of the document in some human-interpretable format.\n",
    "- The `metadata` is relevant information about the document that is provided by the connector via its data source.\n",
    "\n",
    "Below, we can check out the length of our document body to see what's inside, and will probably notice an intractable document length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1703113455184,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
    "outputId": "98b9ef68-c36b-478f-9bbb-1e45b2c49d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents Retrieved: 1\n",
      "Sample of Document 1 Content (Total Length: 89593):\n",
      "From Local to Global: A GraphRAG Approach to\n",
      "Query-Focused Summarization\n",
      "Darren Edge1‚Ä†\n",
      "Ha Trinh1‚Ä†\n",
      "Newman Cheng2\n",
      "Joshua Bradley2\n",
      "Alex Chao3\n",
      "Apurva Mody3\n",
      "Steven Truitt2\n",
      "Dasha Metropolitansky1\n",
      "Robert Osazuwa Ness1\n",
      "Jonathan Larson1\n",
      "1Microsoft Research\n",
      "2Microsoft Strategic Missions and Technologies\n",
      "3Microsoft Office of the CTO\n",
      "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,\n",
      "steventruitt,dasham,robertness,jolarso}@microsoft.com\n",
      "‚Ä†These authors contributed equally to this work\n",
      "Abstract\n",
      "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
      "tion from an external knowledge source enables large language models (LLMs)\n",
      "to answer questions over private and/or previously unseen document collections.\n",
      "However, RAG fails on global questions directed at an entire text corpus, such\n",
      "as ‚ÄúWhat are the main themes in the dataset?‚Äù, since this is inherently a query-\n",
      "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
      "QFS methods, meanwhile, do not scal\n"
     ]
    }
   ],
   "source": [
    "## Printing out a sample of the content\n",
    "print(\"Number of Documents Retrieved:\", len(documents))\n",
    "print(f\"Sample of Document 1 Content (Total Length: {len(documents[0].page_content)}):\")\n",
    "print(documents[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1JjUK2ZSd0HL",
   "metadata": {
    "id": "1JjUK2ZSd0HL"
   },
   "source": [
    "<br>\n",
    "\n",
    "In contrast, the metadata will be much more conservatively-sized to the point of being viable context components for your favorite chat model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Py2lbRXlcX81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1703112982386,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "Py2lbRXlcX81",
    "outputId": "07197dd4-1609-4ecf-ae54-cf6ef3d25458"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-02-19'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Metropolitansky, Robert Osazuwa Ness, Jonathan Larson'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The use of retrieval-augmented generation (RAG) to retrieve relevant\\ninformation from an external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge source enables large language models\\n(LLMs) to answer questions over private and/or previously unseen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">document\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the main themes in the dataset?\", since this is\\ninherently a query-focused summarization (QFS) task, rather than </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an explicit\\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\\ntext indexed by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">typical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose GraphRAG, a graph-based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">approach to question\\nanswering over private text corpora that scales with both the generality of\\nuser questions </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and the quantity of source text. Our approach uses an LLM to\\nbuild a graph index in two stages: first, to derive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for all\\ngroups of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">closely related entities. Given a question, each community summary is\\nused to generate a partial response, before </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">all partial responses are again\\nsummarized in a final response to the user. For a class of global </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sensemaking\\nquestions over datasets in the 1 million token range, we show that GraphRAG\\nleads to substantial </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">improvements over a conventional RAG baseline for both the\\ncomprehensiveness and diversity of generated answers.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2025-02-19'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha \u001b[0m\n",
       "\u001b[32mMetropolitansky, Robert Osazuwa Ness, Jonathan Larson'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The use of retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to retrieve relevant\\ninformation from an external \u001b[0m\n",
       "\u001b[32mknowledge source enables large language models\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to answer questions over private and/or previously unseen \u001b[0m\n",
       "\u001b[32mdocument\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are\u001b[0m\n",
       "\u001b[32mthe main themes in the dataset?\", since this is\\ninherently a query-focused summarization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m task, rather than \u001b[0m\n",
       "\u001b[32man explicit\\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\\ntext indexed by \u001b[0m\n",
       "\u001b[32mtypical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose GraphRAG, a graph-based \u001b[0m\n",
       "\u001b[32mapproach to question\\nanswering over private text corpora that scales with both the generality of\\nuser questions \u001b[0m\n",
       "\u001b[32mand the quantity of source text. Our approach uses an LLM to\\nbuild a graph index in two stages: first, to derive \u001b[0m\n",
       "\u001b[32man entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for all\\ngroups of \u001b[0m\n",
       "\u001b[32mclosely related entities. Given a question, each community summary is\\nused to generate a partial response, before \u001b[0m\n",
       "\u001b[32mall partial responses are again\\nsummarized in a final response to the user. For a class of global \u001b[0m\n",
       "\u001b[32msensemaking\\nquestions over datasets in the 1 million token range, we show that GraphRAG\\nleads to substantial \u001b[0m\n",
       "\u001b[32mimprovements over a conventional RAG baseline for both the\\ncomprehensiveness and diversity of generated answers.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046ea74-0b81-400e-8364-449f421d2add",
   "metadata": {
    "id": "7046ea74-0b81-400e-8364-449f421d2add"
   },
   "source": [
    "<br>\n",
    "\n",
    "Though it may be tempting to accept the metadata format as-is and ignore the body entirely, there are a key selection of features that cannot be approached without diving into the full text:\n",
    "\n",
    "- **The metadata is not guaranteed.** In the case of `arxiv`, paper abstracts, titles, authors, and date are necessary components of a submission, so being able to query them is not surprising. For an arbitrary PDF or webpage though, the same is not necessarily the case.\n",
    "- **The agent will not be able to go deeper into the document content.** The summary is good to know and can be used as-is, but does not provide a straight-forward path to interacting with the body at any capacity (at least not from what we've learned).\n",
    "- **The agent will still not be able to reason about too many documents at once.** Perhaps in the MRKL/ReAct example, you could combine those two summaries into one context and ask some questions. But what happens when you need to interact with 5 documents at once? What about an entire directory? Very soon, you will notice that your context window will be overloaded with information just to summarize or even list out the documents you're interested in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0449e4",
   "metadata": {
    "id": "4e0449e4"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3:** Transforming The Documents\n",
    "\n",
    "Once documents have been loaded, they often need to be transformed if we intend to pass them into our LLMs as context. One method of transformation is known as **chunking**, which breaks down large pieces of content into smaller segments. This technique is valuable because it helps [optimize the relevance of the content returned from the vector database](https://www.pinecone.io/learn/chunking-strategies/).\n",
    "\n",
    "LangChain provides a [variety of document transformers](https://python.langchain.com/docs/integrations/document_transformers/) out of which we will use the [``RecursiveCharacterTextSplitter``](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter). This option will allow us to split our document based on a preference of natural stopping points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1703112527056,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
    "outputId": "a4e666e5-5a5c-413b-f5a4-acca742d80d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "## Some nice custom preprocessing\n",
    "# documents[0].page_content = documents[0].page_content.replace(\". .\", \"\")\n",
    "docs_split = text_splitter.split_documents(documents)\n",
    "\n",
    "# def include_doc(doc):\n",
    "#     ## Some chunks will be overburdened with useless numerical data, so we'll filter it out\n",
    "#     string = doc.page_content\n",
    "#     if len([l for l in string if l.isalpha()]) < (len(string)//2):\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# docs_split = [doc for doc in docs_split if include_doc(doc)]\n",
    "print(len(docs_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1703112530925,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
    "outputId": "1cf24605-65bb-40a2-e7aa-e2d9a8fb6382"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Local to Global: A GraphRAG Approach to\n",
      "Query-Focused Summarization\n",
      "Darren Edge1‚Ä†\n",
      "Ha Trinh1‚Ä†\n",
      "Newman Cheng2\n",
      "Joshua Bradley2\n",
      "Alex Chao3\n",
      "Apurva Mody3\n",
      "Steven Truitt2\n",
      "Dasha Metropolitansky1\n",
      "Robert Osazuwa Ness1\n",
      "Jonathan Larson1\n",
      "1Microsoft Research\n",
      "2Microsoft Strategic Missions and Technologies\n",
      "3Microsoft Office of the CTO\n",
      "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,\n",
      "steventruitt,dasham,robertness,jolarso}@microsoft.com\n",
      "‚Ä†These authors contributed equally to this work\n",
      "Abstract\n",
      "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
      "tion from an external knowledge source enables large language models (LLMs)\n",
      "to answer questions over private and/or previously unseen document collections.\n",
      "However, RAG fails on global questions directed at an entire text corpus, such\n",
      "as ‚ÄúWhat are the main themes in the dataset?‚Äù, since this is inherently a query-\n",
      "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
      "QFS methods, meanwhile, do not scale to the quantities of text indexed by typ-\n",
      "ical RAG systems. To combine the strengths of these contrasting methods, we\n",
      "propose GraphRAG, a graph-based approach to question answering over private\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propose GraphRAG, a graph-based approach to question answering over private\n",
      "text corpora that scales with both the generality of user questions and the quantity\n",
      "of source text. Our approach uses an LLM to build a graph index in two stages:\n",
      "first, to derive an entity knowledge graph from the source documents, then to pre-\n",
      "generate community summaries for all groups of closely related entities. Given a\n",
      "question, each community summary is used to generate a partial response, before\n",
      "all partial responses are again summarized in a final response to the user. For a\n",
      "class of global sensemaking questions over datasets in the 1 million token range,\n",
      "we show that GraphRAG leads to substantial improvements over a conventional\n",
      "RAG baseline for both the comprehensiveness and diversity of generated answers.\n",
      "1\n",
      "Introduction\n",
      "Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using\n",
      "LLMs to answer queries based on data that is too large to contain in a language model‚Äôs context\n",
      "window, meaning the maximum number of tokens (units of text) that can be processed by the LLM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window, meaning the maximum number of tokens (units of text) that can be processed by the LLM\n",
      "at once (Kuratov et al., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to\n",
      "a large external corpus of text records and retrieves a subset of records that are individually relevant\n",
      "to the query and collectively small enough to fit into the context window of the LLM. The LLM then\n",
      "Preprint. Under review.\n",
      "arXiv:2404.16130v2  [cs.CL]  19 Feb 2025\n",
      "generates a response based on both the query and the retrieved records (Baumel et al., 2018; Dang,\n",
      "2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call\n",
      "vector RAG, works well for queries that can be answered with information localized within a small\n",
      "set of records. However, vector RAG approaches do not support sensemaking queries, meaning\n",
      "queries that require global understanding of the entire dataset, such as ‚ÄùWhat are the key trends in\n",
      "how scientific discoveries are influenced by interdisciplinary research over the past decade?‚Äù\n",
      "Sensemaking tasks require reasoning over ‚Äúconnections (which can be among people, places, and\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from few-shot exemplars specialized to those domains.\n",
      "The LLM can also be prompted to extract claims about detected entities. Claims are important\n",
      "factual statements about entities, such as dates, events, and interactions with other entities. As\n",
      "with entities and relationships, in-context learning exemplars can provide domain-specific guidance.\n",
      "Claim descriptions extracted from the example tetx chunk are as follows:\n",
      "‚Ä¢ NeoChip‚Äôs shares surged during their first week of trading on the NewTech Exchange.\n",
      "‚Ä¢ NeoChip debuted as a publicly listed company on the NewTech Exchange.\n",
      "‚Ä¢ Quantum Systems acquired NeoChip in 2016 and held ownership until NeoChip went pub-\n",
      "lic.\n",
      "See Appendix A for prompts and details on our implementation of entity and claim extraction.\n",
      "3.1.3\n",
      "Entities & Relationships ‚ÜíKnowledge Graph\n",
      "The use of an LLM to extract entities, relationships, and claims is a form of abstractive summariza-\n",
      "tion ‚Äì these are meaningful summaries of concepts that, in the case of relationships and claims, may\n",
      "not be explicitly stated in the text. The entity/relationship/claim extraction processes creates mul-\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m-1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.8\n",
      "-3.68\n",
      "0.003\n",
      "C1\n",
      "TS\n",
      "47.92\n",
      "52.08\n",
      "-2.41\n",
      "0.126\n",
      "46.64\n",
      "53.36\n",
      "-2.91\n",
      "0.04\n",
      "C2\n",
      "TS\n",
      "48.8\n",
      "51.2\n",
      "-2.23\n",
      "0.179\n",
      "48.32\n",
      "51.68\n",
      "-2.12\n",
      "0.179\n",
      "C3\n",
      "TS\n",
      "48.08\n",
      "51.92\n",
      "-2.23\n",
      "0.179\n",
      "48.32\n",
      "51.68\n",
      "-2.56\n",
      "0.074\n",
      "C0\n",
      "SS\n",
      "35.12\n",
      "64.88\n",
      "-6.17\n",
      "<0.001\n",
      "41.44\n",
      "58.56\n",
      "-4.82\n",
      "<0.001\n",
      "C1\n",
      "SS\n",
      "40.32\n",
      "59.68\n",
      "-4.83\n",
      "<0.001\n",
      "45.2\n",
      "54.8\n",
      "-3.19\n",
      "0.017\n",
      "C2\n",
      "SS\n",
      "40.4\n",
      "59.6\n",
      "-4.67\n",
      "<0.001\n",
      "44.88\n",
      "55.12\n",
      "-3.65\n",
      "0.003\n",
      "C3\n",
      "SS\n",
      "40.48\n",
      "59.52\n",
      "-4.69\n",
      "<0.001\n",
      "45.6\n",
      "54.4\n",
      "-2.86\n",
      "0.043\n",
      "TS\n",
      "SS\n",
      "43.6\n",
      "56.4\n",
      "-3.96\n",
      "<0.001\n",
      "46\n",
      "54\n",
      "-2.68\n",
      "0.066\n",
      "C0\n",
      "C1\n",
      "46.96\n",
      "53.04\n",
      "-2.87\n",
      "0.037\n",
      "47.6\n",
      "52.4\n",
      "-2.17\n",
      "0.179\n",
      "C0\n",
      "C2\n",
      "48.4\n",
      "51.6\n",
      "-2.06\n",
      "0.197\n",
      "48.48\n",
      "51.52\n",
      "-1.61\n",
      "0.321\n",
      "C1\n",
      "C2\n",
      "49.84\n",
      "50.16\n",
      "-1\n",
      "0.952\n",
      "49.28\n",
      "50.72\n",
      "-1.6\n",
      "0.321\n",
      "C0\n",
      "C3\n",
      "48.4\n",
      "51.6\n",
      "-1.8\n",
      "0.29\n",
      "47.2\n",
      "52.8\n",
      "-2.62\n",
      "0.071\n",
      "C1\n",
      "C3\n",
      "49.76\n",
      "50.24\n",
      "0\n",
      "1\n",
      "48.8\n",
      "51.2\n",
      "-1.29\n",
      "0.321\n",
      "C2\n",
      "C3\n",
      "50\n",
      "50\n",
      "0\n",
      "1\n",
      "48.8\n",
      "51.2\n",
      "-1.84\n",
      "0.262\n",
      "26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in (0, 1, 2, 15, -1):\n",
    "    pprint(f\"[Document {i}]\")\n",
    "    print(docs_split[i].page_content)\n",
    "    pprint(\"=\"*64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2f969-72cd-4d0e-a150-e3efafc1cdfc",
   "metadata": {
    "id": "57e2f969-72cd-4d0e-a150-e3efafc1cdfc"
   },
   "source": [
    "<br>\n",
    "\n",
    "Our approach for chunking is pretty naive, but highlights the ease of getting at least something working for our application. We made some effort to keep the chunk size small so that our models are able to wield it effectively as context, but how are we going to reason about all of these pieces?\n",
    "\n",
    "**When extending and optimizing this approach for an arbitrary set of documents, some potential options include:**\n",
    "\n",
    "- Identifying logical breaks or synthesis techniques (manually, automatically, LLM-assisted, etc).\n",
    "- Aiming to construct chunks that are rich in unique and relevant information, avoiding redundancy to maximize database utility.\n",
    "- Customizing chunking to fit the document‚Äôs nature, ensuring the chunks are contextually relevant and cohesive.\n",
    "- Including key concepts, keywords, or metadata snippets in each chunk for improved searchability and relevance in the database.\n",
    "- Continuously assessing chunking effectiveness and be ready to adjust strategies for optimal balance between size and content richness.\n",
    "- Considering a hierarchy system (implicitly-generated or explicitly-specified) to improve retrieval attempts.\n",
    "    - If interested, please look over the [**LlamaIndex tree structures from the index guide**](https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide.html#tree-index) as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-0QApYgNbyJD",
   "metadata": {
    "id": "-0QApYgNbyJD"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4: [Exercise]** Refining Summaries\n",
    "\n",
    "To automatically reason about large documents, one potential idea might be to use LLMs to create a dense summary or knowledge base. Similar to how we maintained a running history of the conversation via slot-filling in the previous notebook, is there any problem with keeping a running history of an entire document?\n",
    "\n",
    "In this section, we focus on an exciting application of LLMs: **automatically refining, coercing, and consolidating data en masse**. Specifically, we'll be implementing a simple but useful Runnable that uses a while loop and the running state chain formulation to summarize a set of document chunks. This process is commonly known as [**\"document refinement\"**](https://js.langchain.com/v0.1/docs/modules/chains/document/refine/) and is largely akin to our previous conversation-focused slot-filling exercise; the only difference is that now we're dealing with a large document instead of a growing chat history.\n",
    "\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1J2XR8Cc8YSkVJMiJCknMkgA02mBT8riZ\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_refine.png\" width=1000px/>\n",
    ">\n",
    "> From [**Refine | LangChain**ü¶úÔ∏èüîó](https://js.langchain.com/v0.1/docs/modules/chains/document/refine/)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **The DocumentSummaryBase Model**\n",
    "\n",
    "Much like the `KnowledgeBase` class from the previous notebook, we can create a `DocumentSummaryBase` structure designed to encapsulate the essence of a document. The one below will use the `running_summary` field to query the model for a final summary while attempting to use the `main_ideas` and `loose_ends` fields as a bottleneck to keep the running summary from moving too fast. This is something we're going to have to enforce via prompt engineering, so the `summary_prompt` is also provided which shows how this information will be used. Feel free to modify it as necessary to make it work for your model of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gE8y2JvLvZ5T",
   "metadata": {
    "id": "gE8y2JvLvZ5T"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class DocumentSummaryBase(BaseModel):\n",
    "    running_summary: str = Field(\"\", description=\"Running description of the document. Do not override; only update!\")\n",
    "    main_ideas: List[str] = Field([], description=\"Most important information from the document (max 3)\")\n",
    "    loose_ends: List[str] = Field([], description=\"Open questions that would be good to incorporate into summary, but that are yet unknown (max 3)\")\n",
    "\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are generating a running summary of the document. Make it readable by a technical user.\"\n",
    "    \" After this, the old knowledge base will be replaced by the new one. Make sure a reader can still understand everything.\"\n",
    "    \" Keep it short, but as dense and useful as possible! The information should flow from chunk to (loose ends or main ideas) to running_summary.\"\n",
    "    \" The updated knowledge base keep all of the information from running_summary here: {info_base}.\"\n",
    "    \"\\n\\n{format_instructions}. Follow the format precisely, including quotations and commas\"\n",
    "    \"\\n\\nWithout losing any of the info, update the knowledge base with the following: {input}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7LkjfpOAvlEd",
   "metadata": {
    "id": "7LkjfpOAvlEd"
   },
   "source": [
    "<br>\n",
    "\n",
    "We will also use this opportunity to bring back the `RExtract` function from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "khRhVghHxBaz",
   "metadata": {
    "id": "khRhVghHxBaz"
   },
   "outputs": [],
   "source": [
    "def RExtract(pydantic_class, llm, prompt):\n",
    "    '''\n",
    "    Runnable Extraction module\n",
    "    Returns a knowledge dictionary populated by slot-filling extraction\n",
    "    '''\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
    "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
    "    def preparse(string):\n",
    "        if '{' not in string: string = '{' + string\n",
    "        if '}' not in string: string = string + '}'\n",
    "        string = (string\n",
    "            .replace(\"\\\\_\", \"_\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\"\\]\", \"]\")\n",
    "            .replace(\"\\[\", \"[\")\n",
    "        )\n",
    "        # print(string)  ## Good for diagnostics\n",
    "        return string\n",
    "    return instruct_merge | prompt | llm | preparse | parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFtME_s4PRoW",
   "metadata": {
    "id": "oFtME_s4PRoW"
   },
   "source": [
    "<br>\n",
    "\n",
    "With this in mind, the following code invokes the running state chain in a for-loop to iterate over your documents! The only modification necessary should be the `parse_chain` implementation, which should pass the the state through a properly-configured `RExtract` chain from the last notebook. After this, the system should work decently to maintain a running summary of the document (though some tweaking of the prompt may be required depending on the model used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6sODIfHUgz6m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79192,
     "status": "ok",
     "timestamp": 1703112894722,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6sODIfHUgz6m",
    "outputId": "7b5aee70-078b-458e-d2a7-e8601b789fb1"
   },
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Invalid json output: {   \"running_summary\": \"GraphRAG is a graph-based approach to question answering that scales with the generality of user questions and the quantity of source text. It uses an LLM to build a graph index in two stages: deriving an entity knowledge graph from source documents and pre-generating community summaries for groups of closely related entities. These community summaries are generated in a bottom-up manner following the hierarchical structure of extracted communities, with higher-level summaries incorporating lower-level ones. Given a question, each community summary is used to generate a partial response, before all partial responses are summarized into a final response using map-reduce processing. GraphRAG improves upon a conventional RAG baseline for both the comprehensiveness and diversity of generated answers for global sensemaking questions over large datasets (1 million tokens). In contrast to canonical RAG approaches, where retrieved records populate a prompt template that is passed to the LLM, GraphRAG employs a graph-based mechanism to handle queries that require global sensemaking over the entire data corpus. GraphRAG is inspired by prior work on advanced RAG strategies and leverages summaries over large sections of the data source as a form of 'self-memory' (Cheng et al. 2024). Results show GraphRAG strongly outperforms vector RAG when using GPT-4 as the LLM. It is available as open-source software and as extensions to multiple open-source libraries.\",   \"main_ideas\": [     \"GraphRAG is a graph-based approach that scales with both the generality of user questions and the quantity of source text.\",     \"GraphRAG uses an LLM to build a graph index in two stages: deriving an entity knowledge graph from source documents and pre-generating community summaries for groups of closely related entities.\",     \"GraphRAG generates community summaries in a bottom-up manner, with higher-level summaries incorporating lower-level ones.\",     \"Using a second LLM to judge answers, GraphRAG outperforms vector RAG for global sensemaking questions over large datasets. Compared to canonical RAG approaches, GraphRAG uses a graph-based mechanism inspired by advanced RAG strategies, leveraging summaries over large sections of the data source as a form of 'self-memory' (Cheng et al. 2024)\"   ],   \"loose_ends\": [     \"How does GraphRAG overcome the limitations of the LLM's context window compared to conventional RAG approaches?\",     'What are the specific ways GraphRAG improves the scalability of question answering methods?',     'How does GraphRAG apply the LLM-as-a-judge technique (Zheng et al., 2024) specifically?',     'What corpus-specific use cases are used for comparing GraphRAG to vector RAG?',     'What is the nature of the prompt template used in canonical RAG approaches, and how does it differ from GraphRAG's approach?'   ] }\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/output_parsers/json.py:83\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/utils/json.py:145\u001b[39m, in \u001b[36mparse_json_markdown\u001b[39m\u001b[34m(json_string, parser)\u001b[39m\n\u001b[32m    144\u001b[39m     json_str = json_string \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m match.group(\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/utils/json.py:161\u001b[39m, in \u001b[36m_parse_json\u001b[39m\u001b[34m(json_str, parser)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/utils/json.py:119\u001b[39m, in \u001b[36mparse_partial_json\u001b[39m\u001b[34m(s, strict)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/json/__init__.py:359\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    358\u001b[39m     kw[\u001b[33m'\u001b[39m\u001b[33mparse_constant\u001b[39m\u001b[33m'\u001b[39m] = parse_constant\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/json/decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 2469 (char 2468)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOutputParserException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m## Take the first 10 document chunks and accumulate a DocumentSummaryBase\u001b[39;00m\n\u001b[32m     47\u001b[39m summarizer = RSummarizer(DocumentSummaryBase(), instruct_llm, summary_prompt, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m summary = \u001b[43msummarizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs_split\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:4719\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4705\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[32m   4706\u001b[39m \n\u001b[32m   4707\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4716\u001b[39m \u001b[33;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[32m   4717\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4720\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4721\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4722\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4723\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4724\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4725\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4726\u001b[39m     msg = (\n\u001b[32m   4727\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4728\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4729\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:1925\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1921\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1922\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1923\u001b[39m         output = cast(\n\u001b[32m   1924\u001b[39m             Output,\n\u001b[32m-> \u001b[39m\u001b[32m1925\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1927\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1928\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1933\u001b[39m         )\n\u001b[32m   1934\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1935\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:4573\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4571\u001b[39m                 output = chunk\n\u001b[32m   4572\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4573\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4574\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4575\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4576\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mRSummarizer.<locals>.summarize_docs\u001b[39m\u001b[34m(docs)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(docs):\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m## TODO: Update the state as appropriate using your parse_chain component\u001b[39;00m\n\u001b[32m     26\u001b[39m     state[\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m] = doc.page_content  \u001b[38;5;66;03m# Add current document content\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     state = \u001b[43mparse_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Extract and update info_base\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33minfo_base\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state \n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py:496\u001b[39m, in \u001b[36mRunnableAssign.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    491\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    492\u001b[39m     \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    493\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    494\u001b[39m     **kwargs: Any,\n\u001b[32m    495\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:1925\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1921\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1922\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1923\u001b[39m         output = cast(\n\u001b[32m   1924\u001b[39m             Output,\n\u001b[32m-> \u001b[39m\u001b[32m1925\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1927\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1928\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1933\u001b[39m         )\n\u001b[32m   1934\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1935\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py:483\u001b[39m, in \u001b[36mRunnableAssign._invoke\u001b[39m\u001b[34m(self, input, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    482\u001b[39m     **\u001b[38;5;28minput\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     **\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    488\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:3728\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3723\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3724\u001b[39m         futures = [\n\u001b[32m   3725\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3726\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3727\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m3728\u001b[39m         output = \u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m   3729\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3730\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:3728\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   3723\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3724\u001b[39m         futures = [\n\u001b[32m   3725\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3726\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3727\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m3728\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   3729\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3730\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:3712\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input, config, key)\u001b[39m\n\u001b[32m   3706\u001b[39m child_config = patch_config(\n\u001b[32m   3707\u001b[39m     config,\n\u001b[32m   3708\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3709\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3710\u001b[39m )\n\u001b[32m   3711\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3712\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m context.run(\n\u001b[32m   3713\u001b[39m         step.invoke,\n\u001b[32m   3714\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3715\u001b[39m         child_config,\n\u001b[32m   3716\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:3025\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   3024\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3025\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[32m   3026\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3027\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:202\u001b[39m, in \u001b[36mBaseOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result(\n\u001b[32m    195\u001b[39m             [ChatGeneration(message=inner_input)]\n\u001b[32m   (...)\u001b[39m\u001b[32m    199\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    200\u001b[39m     )\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:1925\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1921\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1922\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1923\u001b[39m         output = cast(\n\u001b[32m   1924\u001b[39m             Output,\n\u001b[32m-> \u001b[39m\u001b[32m1925\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1927\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1928\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1933\u001b[39m         )\n\u001b[32m   1934\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1935\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:203\u001b[39m, in \u001b[36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result(\n\u001b[32m    195\u001b[39m             [ChatGeneration(message=inner_input)]\n\u001b[32m   (...)\u001b[39m\u001b[32m    199\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    200\u001b[39m     )\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    204\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    205\u001b[39m         config,\n\u001b[32m    206\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    207\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/output_parsers/pydantic.py:67\u001b[39m, in \u001b[36mPydanticOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse the result of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[32m     55\u001b[39m \n\u001b[32m     56\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m \u001b[33;03m    The parsed pydantic object.\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     json_object = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_obj(json_object)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/langchain_core/output_parsers/json.py:86\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     85\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output=text) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOutputParserException\u001b[39m: Invalid json output: {   \"running_summary\": \"GraphRAG is a graph-based approach to question answering that scales with the generality of user questions and the quantity of source text. It uses an LLM to build a graph index in two stages: deriving an entity knowledge graph from source documents and pre-generating community summaries for groups of closely related entities. These community summaries are generated in a bottom-up manner following the hierarchical structure of extracted communities, with higher-level summaries incorporating lower-level ones. Given a question, each community summary is used to generate a partial response, before all partial responses are summarized into a final response using map-reduce processing. GraphRAG improves upon a conventional RAG baseline for both the comprehensiveness and diversity of generated answers for global sensemaking questions over large datasets (1 million tokens). In contrast to canonical RAG approaches, where retrieved records populate a prompt template that is passed to the LLM, GraphRAG employs a graph-based mechanism to handle queries that require global sensemaking over the entire data corpus. GraphRAG is inspired by prior work on advanced RAG strategies and leverages summaries over large sections of the data source as a form of 'self-memory' (Cheng et al. 2024). Results show GraphRAG strongly outperforms vector RAG when using GPT-4 as the LLM. It is available as open-source software and as extensions to multiple open-source libraries.\",   \"main_ideas\": [     \"GraphRAG is a graph-based approach that scales with both the generality of user questions and the quantity of source text.\",     \"GraphRAG uses an LLM to build a graph index in two stages: deriving an entity knowledge graph from source documents and pre-generating community summaries for groups of closely related entities.\",     \"GraphRAG generates community summaries in a bottom-up manner, with higher-level summaries incorporating lower-level ones.\",     \"Using a second LLM to judge answers, GraphRAG outperforms vector RAG for global sensemaking questions over large datasets. Compared to canonical RAG approaches, GraphRAG uses a graph-based mechanism inspired by advanced RAG strategies, leveraging summaries over large sections of the data source as a form of 'self-memory' (Cheng et al. 2024)\"   ],   \"loose_ends\": [     \"How does GraphRAG overcome the limitations of the LLM's context window compared to conventional RAG approaches?\",     'What are the specific ways GraphRAG improves the scalability of question answering methods?',     'How does GraphRAG apply the LLM-as-a-judge technique (Zheng et al., 2024) specifically?',     'What corpus-specific use cases are used for comparing GraphRAG to vector RAG?',     'What is the nature of the prompt template used in canonical RAG approaches, and how does it differ from GraphRAG's approach?'   ] }\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "latest_summary = \"\"\n",
    "\n",
    "## TODO: Use the techniques from the previous notebook to complete the exercise\n",
    "def RSummarizer(knowledge, llm, prompt, verbose=False):\n",
    "    '''\n",
    "    Exercise: Create a chain that summarizes\n",
    "    '''\n",
    "    ###########################################################################################\n",
    "    ## START TODO:\n",
    "\n",
    "    def summarize_docs(docs):        \n",
    "        ## TODO: Initialize the parse_chain appropriately; should include an RExtract instance.\n",
    "        ## HINT: You can get a class using the <object>.__class__ attribute...\n",
    "        parse_chain = RunnableAssign({'info_base': RExtract(knowledge.__class__, llm=llm, prompt=prompt)})\n",
    "        \n",
    "        ## TODO: Initialize a valid starting state. Should be similar to notebook 4\n",
    "        state = {\n",
    "            'info_base': knowledge,  # Start with the initial knowledge object\n",
    "            'input': \"\"  # Will hold current document content\n",
    "        }\n",
    "        \n",
    "        global latest_summary  ## If your loop crashes, you can check out the latest_summary\n",
    "        \n",
    "        for i, doc in enumerate(docs):\n",
    "            ## TODO: Update the state as appropriate using your parse_chain component\n",
    "            state['input'] = doc.page_content  # Add current document content\n",
    "            state = parse_chain.invoke(state)  # Extract and update info_base\n",
    "            \n",
    "            assert 'info_base' in state \n",
    "            if verbose:\n",
    "                print(f\"Considered {i+1} documents\")\n",
    "                pprint(state['info_base'])\n",
    "                latest_summary = state['info_base']\n",
    "                clear_output(wait=True)\n",
    "        return state['info_base']\n",
    "        \n",
    "    ## END TODO\n",
    "    ###########################################################################################\n",
    "    \n",
    "    return RunnableLambda(summarize_docs)\n",
    "\n",
    "# instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\").bind(max_tokens=4096)\n",
    "instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\").bind(max_tokens=4096)\n",
    "instruct_llm = instruct_model | StrOutputParser()\n",
    "\n",
    "## Take the first 10 document chunks and accumulate a DocumentSummaryBase\n",
    "summarizer = RSummarizer(DocumentSummaryBase(), instruct_llm, summary_prompt, verbose=True)\n",
    "summary = summarizer.invoke(docs_split[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07eb5710-23f7-4782-84eb-1fc8f73500b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG is a graph-based approach to question answering that scales with the generality of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">user questions and the quantity of source text. It uses an LLM to build a graph index in two stages: deriving an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entity knowledge graph from source documents and pre-generating community summaries for groups of closely related </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entities. These community summaries are generated in a bottom-up manner following the hierarchical structure of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracted communities, with higher-level summaries incorporating lower-level ones. Given a question, each community</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">summary is used to generate a partial response, before all partial responses are summarized into a final response </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using map-reduce processing. GraphRAG improves upon a conventional RAG baseline for both the comprehensiveness and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">diversity of generated answers for global sensemaking questions over large datasets (1 million tokens). To </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">demonstrate its ability for global sensemaking, GraphRAG uses a diverse set of global sensemaking questions based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on corpus-specific use cases, before using a second LLM to judge the answers of two different RAG systems using </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">predefined criteria (defined in Section 3.3). Results show GraphRAG strongly outperforms vector RAG when using </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">GPT-4 as the LLM. It is available as open-source software and as extensions to multiple open-source libraries.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG is a graph-based approach that scales with both the generality of user questions and the quantity</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of source text.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG uses an LLM to build a graph index in two stages: deriving an entity knowledge graph from source </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documents and pre-generating community summaries for groups of closely related entities.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG generates community summaries in a bottom-up manner, with higher-level summaries incorporating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lower-level ones.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Using a second LLM to judge answers, GraphRAG outperforms vector RAG for global sensemaking questions over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">large datasets.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">\"How does GraphRAG overcome the limitations of the LLM's context window compared to conventional RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">approaches?\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What are the specific ways GraphRAG improves the scalability of question answering methods?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG apply the LLM-as-a-judge technique (Zheng et al., 2024) specifically?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What corpus-specific use cases are used for comparing GraphRAG to vector RAG?'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'GraphRAG is a graph-based approach to question answering that scales with the generality of \u001b[0m\n",
       "\u001b[32muser questions and the quantity of source text. It uses an LLM to build a graph index in two stages: deriving an \u001b[0m\n",
       "\u001b[32mentity knowledge graph from source documents and pre-generating community summaries for groups of closely related \u001b[0m\n",
       "\u001b[32mentities. These community summaries are generated in a bottom-up manner following the hierarchical structure of \u001b[0m\n",
       "\u001b[32mextracted communities, with higher-level summaries incorporating lower-level ones. Given a question, each community\u001b[0m\n",
       "\u001b[32msummary is used to generate a partial response, before all partial responses are summarized into a final response \u001b[0m\n",
       "\u001b[32musing map-reduce processing. GraphRAG improves upon a conventional RAG baseline for both the comprehensiveness and \u001b[0m\n",
       "\u001b[32mdiversity of generated answers for global sensemaking questions over large datasets \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1 million tokens\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. To \u001b[0m\n",
       "\u001b[32mdemonstrate its ability for global sensemaking, GraphRAG uses a diverse set of global sensemaking questions based \u001b[0m\n",
       "\u001b[32mon corpus-specific use cases, before using a second LLM to judge the answers of two different RAG systems using \u001b[0m\n",
       "\u001b[32mpredefined criteria \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdefined in Section 3.3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Results show GraphRAG strongly outperforms vector RAG when using \u001b[0m\n",
       "\u001b[32mGPT-4 as the LLM. It is available as open-source software and as extensions to multiple open-source libraries.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG is a graph-based approach that scales with both the generality of user questions and the quantity\u001b[0m\n",
       "\u001b[32mof source text.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG uses an LLM to build a graph index in two stages: deriving an entity knowledge graph from source \u001b[0m\n",
       "\u001b[32mdocuments and pre-generating community summaries for groups of closely related entities.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG generates community summaries in a bottom-up manner, with higher-level summaries incorporating \u001b[0m\n",
       "\u001b[32mlower-level ones.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Using a second LLM to judge answers, GraphRAG outperforms vector RAG for global sensemaking questions over\u001b[0m\n",
       "\u001b[32mlarge datasets.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m\"How does GraphRAG overcome the limitations of the LLM's context window compared to conventional RAG \u001b[0m\n",
       "\u001b[32mapproaches?\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What are the specific ways GraphRAG improves the scalability of question answering methods?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG apply the LLM-as-a-judge technique \u001b[0m\u001b[32m(\u001b[0m\u001b[32mZheng et al., 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m specifically?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What corpus-specific use cases are used for comparing GraphRAG to vector RAG?'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(latest_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tKtoLf6DPv4Z",
   "metadata": {
    "id": "tKtoLf6DPv4Z"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5:** Synthetic Data Processing\n",
    "\n",
    "As we conclude our exploration of document summarization using LLMs, it's important to acknowledge the broader context and potential challenges. While we've demonstrated a viable method for extracting concise, meaningful summaries, let's consider why such an approach is crucial and the complexities it entails.\n",
    "\n",
    "#### **Generality of Refinement**\n",
    "\n",
    "It's important to note that this \"progressive summarization\" technique is merely a starter chain that makes few assumptions about the initial data and desired output format. The same technique can be expanded far and wide to generate synthetic refinements with known metadata, active assumptions, and downstream objectives in mind.\n",
    "\n",
    "**Consider these potential applications:**\n",
    "\n",
    "1. **Aggregating Data**: Constructing structures that transform raw data from document chunks into coherent, useful summaries.\n",
    "2. **Categorization and Sub-topic Analysis**: Creating systems that categorize insights from chunks into defined categories, tracking emerging sub-topics within each.\n",
    "3. **Consolidation into Dense Informational Chunks**: Refining these structures to distill insights into compact segments, enriched with direct quotes for deeper analysis.\n",
    "\n",
    "These applications hint at the creation of a **domain-specific knowledge graph** which can be accessed and traversed by a conversational chat model. Some utilities already exist to generate these automatically via tools like [**LangChain Knowledge Graphs**](https://python.langchain.com/docs/tutorials/graph/). Though you might need to develop hierarchical structures and tools to both construct and traverse such a structure, it is a viable option when you can properly refine a sufficient knowledge graph for your use case! For this intereted in more advanced knowledge graph construction techniques which rely on larger systems and vector similarity, we found the [**LangChain x Neo4j Article**](https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/) to be of interest.\n",
    "\n",
    "### **Addressing the Challenges of Large-Scale Data Processing**\n",
    "\n",
    "While our approach opens up exciting possibilities, it's not without its challenges, especially when dealing with large volumes of data:\n",
    "\n",
    "- **Generic Preprocessing Limitations**: While summarization is relatively straightforward, developing hierarchies that are universally effective across various contexts is challenging.\n",
    "\n",
    "- **Granularity and Navigation Costs**: Achieving detailed granularity in a hierarchy can be resource-intensive, requiring sophisticated consolidation or extensive branching to maintain manageable context sizes per interaction.\n",
    "\n",
    "- **Dependency on Precise Instruction Execution**: Navigating such a hierarchy with our current tools would rely heavily on powerful instruction-tuned models with strong prompt engineering. The inference latency and the risk of errors in argument prediction can be significant, so using LLMs for this could be a challenge.\n",
    "\n",
    "As you progress through the course, keep track of how these challenges get addressed with subsequent techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdFSMXOVRzEa",
   "metadata": {
    "id": "cdFSMXOVRzEa"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 6:** Wrap-Up\n",
    "\n",
    "The goal of this notebook was to introduce the problems and techniques surrounding large document handling for chat models. In the next notebook, we will investigate a complementary tool with a very different set of pros and cons; **semantic retrieval with embedding models.**\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "2. **[Optional]** This notebooks includes some fundamental document processing chains, but does not touch upon [Map Reduce](https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/) chains which are also very useful and build on roughly the same intuitions. These are a good next step, so please check them out!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4",
   "metadata": {
    "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
