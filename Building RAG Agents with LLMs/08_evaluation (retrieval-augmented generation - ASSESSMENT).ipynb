{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [Assessment]:** RAG Evaluation</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Welcome to the last notebook of the course! In the previous notebook, you integrated a vector store solution into a RAG pipeline! In this notebook, you will take that same pipeline and evaluate it using numerical RAG evaluation techniques incorporating LLM-as-a-Judge metrics!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learn how to integrate the techniques from prior notebooks to numerically approximate the goodness of your RAG pipeline.\n",
    "\n",
    "- **Final Exercice**: ***By working through this notebook in the Course Environment,* you will be able to submit the coding component of the course!**\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- As you go along, remember what our metrics actually represent. Should our pipeline pass these objectives? Is our judge LLM sufficient for evaluating the pipeline? Does a particular metric even matter for our use case?\n",
    "- If we left the vectorstore-as-a-memory component in our chain, do you think it would still pass the evaluation? Additionally, is the evaluation useful for assessing vectorstore-as-a-memory performance? \n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Pre-Release Evaluation\n",
    "\n",
    "In our previous notebook, we successfully combined several concepts to create a document chatbot with the aim of responsive and informative interactions. However, the diversity of user interactions necessitates comprehensive testing to truly understand the chatbot's performance. Thorough testing in varied scenarios is crucial to ensure that the system is not only robust and versatile but also aligns with user and provider expectations.\n",
    "\n",
    "After defining your chatbot's roles and implementing the necessary features, evaluating it becomes a multi-stage process:\n",
    "\n",
    "- **Typical Use Inspection:** Start by testing scenarios most relevant to your use case. See if your chatbot can reliably navigate discussions with limited human intervention.\n",
    "\n",
    "    - Additionally, identify limitations or compartments that should be redirected to a human for inspection/supervision (i.e., human swap-in to confirm transactions or perform sensitive navigation) and implement those options.\n",
    "\n",
    "- **Edge Case Inspection:** Explore the boundaries of typical use, identifying how the chatbot handles less common but plausible scenarios.\n",
    "\n",
    "    - Before any public release, assess critical boundary conditions that could pose liability risks, such as the potential generation of inappropriate content.\n",
    "\n",
    "    - Implement well-tested guardrails on all outputs (and possibly inputs) to limit undesired interactions and redirect users into predictable conversation flows.\n",
    "\n",
    "- **Progressive Rollout:** Rolling out your model to a limited audience (first internal, then [A/B](https://en.wikipedia.org/wiki/A/B_testing)) and implement analytics features like usage analytics dashboards and feedback avenues (flag/like/dislike/etc).\n",
    "\n",
    "Of these three steps, the first two can be done by a small team or an individual and should be iterated on as part of the development process. Unfortunately, this needs to be done frequently and can be prone to human error. **Luckily for us, LLMs can be used to help out with LLM-as-a-Judge formulations!**\n",
    "\n",
    "*(Yeah, this probably isn't surprising by now. LLMs being strong is why this course is here...).*\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** LLM-as-a-Judge Formulation\n",
    "\n",
    "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
    "\n",
    "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
    "\n",
    "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
    "\n",
    "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
    "\n",
    "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale.\n",
    "\n",
    "**There are several popular frameworks for off-the-shelf judge formulations including:**\n",
    "- [**RAGAs (short for RAG Assessment)**](https://docs.ragas.io/en/stable/), which offers a suite of great starting points for your own evaluation efforts.\n",
    "- [**LangChain Evaluators**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/), which are similar first-party options with many implicitly-constructible agents.\n",
    "\n",
    "Instead of using the chains as-is, we will instead expand on the ideas and evaluate our system with a more custom solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3: [Assessment Prep]** Pairwise Evaluator\n",
    "\n",
    "The following exercise will flesh out a custom implementation of a simplified [LangChain Pairwise String Evaluator](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/). \n",
    "\n",
    "**To prepare for our RAG chain evaluation, we will need to:**\n",
    "\n",
    "- Pull in our document index (the one we saved in the previous notebook).\n",
    "- Recreate our RAG pipeline of choice.\n",
    "\n",
    "**We will specifically be implementing a judge formulation with the following steps:**\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "**The chain should be a simple but powerful process that tests for the following objective:**\n",
    "\n",
    "> ***Does my RAG chain outperform a narrow chatbot with limited document access.***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This will be the system used for the final evaluation!** To see how this system is integrated into the autograder, please check out the implementation in [`frontend/server_app.py`](frontend/server_app.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1:** Pull In Your Document Retrieval Index\n",
    "\n",
    "For this exercise, you will pull in the `docstore_index` file you created as part of your earlier notebook. The following cell should be able to load in the store as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">253</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m253\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling\n",
      "\n",
      "Summary: The proliferation of tool-augmented Large Language Models (LLMs) has created\n",
      "a fragmented ecosystem where developers must navigate multiple protocols,\n",
      "manual schema definitions, and complex execution workflows. We address this\n",
      "challenge by proposing a unified approach to tool integration that abstracts\n",
      "protocol differences while optimizing execution performance. Our solution\n",
      "demonstrates how protocol-agnostic design principles can significantly reduce\n",
      "development overhead through automated schema generation, dual-mode concurrent\n",
      "execution, and seamless multi-source tool management. Experimental results show\n",
      "60-80% code reduction across integration scenarios, performance improvements up\n",
      "to 3.1x through optimized concurrency, and full compatibility with existing\n",
      "function calling standards. This work contributes both theoretical insights\n",
      "into tool integration architecture and practical solutions for real-world LLM\n",
      "application development.\n",
      "\n",
      "Page Body: . 2023) manage tool co-\\nordination at a higher level of abstraction.\\nIn practice, these paradigms increasingly converge, par-\\nticularly in production environments where in-context learn-\\ning forms the backbone of most implementations. Even\\nfine-tuned models typically rely on the LLM\\u2019s native in-\\ncontext capabilities for core tool selection decisions, oper-\\nating through standardized function-calling interfaces from\\nmajor providers. This practical convergence creates both\\nopportunities and challenges - while enabling flexible tool\\ncomposition, it also introduces complexity in managing het-\\nerogeneous tool descriptions, context window limitations,\\nand response formats across different platforms.\\nProtocol Standardization Challenges\\nFunction Calling Standards\\nThe industry has converged\\npragmatically on in-context learning implementations for\\ntool calling across major LLM APIs, including OpenAI, An-\\nthropic, and Google\n"
     ]
    }
   ],
   "source": [
    "## Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/embed-qa-4\", truncate=\"END\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout just confirms that your store has been retrieved\n",
    "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"Sample Chunk:\")\n",
    "print(format_chunk(docs[len(docs)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2: [Exercise]** Pull In Your RAG Chain\n",
    "\n",
    "Now that we have our index, we can recreate the RAG agent from the previous notebook! \n",
    "\n",
    "**Key Modifications:**\n",
    "- To keep things simple, feel free to disregard the vectorstore-as-a-memory component. Incorporating it will require some more overhead and will make the exercise a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a great question! Did you know that we have evidence that fully autonomous AI agents can implement the core functionality of issues on several open-source repositories? This is a massive leap forward in AI development and has the potential to significantly speed up development in this setting.\n",
      "\n",
      "According to the study in [\"Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity\"], these AI agents were able to correctly implement the core functionality of issues, although they failed to fully satisfy all requirements. This is a big deal, and if progress continues, we might see significant speedups in open-source development.\n",
      "\n",
      "It's also worth noting that the repositories used in this study are extremely large and mature, with an average of 1.1 million lines of code and 23,000 stars. Still, the AI agents were able to make meaningful progress on these complex projects. Pretty cool, huh?"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    ")\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "#####################################################################\n",
    "## TODO: Pull in your desired RAG Chain. Memory not necessary\n",
    "\n",
    "## Chain 1 Specs: \"Hello World\" -> retrieval_chain \n",
    "##   -> {'input': <str>, 'context' : <str>}\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
    "\n",
    "# Context getter: extract input from dict, then retrieve docs and convert to string\n",
    "context_getter = (\n",
    "    itemgetter('input')  # Extract the input string from the dictionary\n",
    "    | docstore.as_retriever() \n",
    "    | long_reorder \n",
    "    | docs2str\n",
    ")\n",
    "\n",
    "retrieval_chain = {'input': (lambda x: x)} | RunnableAssign({'context': context_getter})\n",
    "\n",
    "## Chain 2 Specs: retrieval_chain -> generator_chain \n",
    "##   -> {\"output\" : <str>, ...} -> output_puller\n",
    "generator_chain = chat_prompt | llm\n",
    "generator_chain = {'output': generator_chain} | RunnableLambda(output_puller)  ## GIVEN\n",
    "\n",
    "\n",
    "## END TODO\n",
    "#####################################################################\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
    "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Step 3:** Generating Synthetic Question-Answer Pairs\n",
    "\n",
    "In this section, we can implement the first few part of our evaluation routine:\n",
    "\n",
    "- **Sample the RAG agent document pool to find two document chunks.**\n",
    "- **Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.**\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How do the AI tools used in the context of the study on experienced open-source developers (</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Cursor Pro\"</span><span style=\"font-weight: bold\"> </span>\n",
       "<span style=\"font-weight: bold\">and </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Claude 3.5/3.7 Sonnet\"</span><span style=\"font-weight: bold\">) compare to the type of AI modeled in the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"font-weight: bold\"> framework for large language </span>\n",
       "<span style=\"font-weight: bold\">model (LLM) inference?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How do the AI tools used in the context of the study on experienced open-source developers \u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"Cursor Pro\"\u001b[0m\u001b[1m \u001b[0m\n",
       "\u001b[1mand \u001b[0m\u001b[32m\"Claude 3.5/3.7 Sonnet\"\u001b[0m\u001b[1m)\u001b[0m\u001b[1m compare to the type of AI modeled in the \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1m framework for large language \u001b[0m\n",
       "\u001b[1mmodel \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLM\u001b[0m\u001b[1m)\u001b[0m\u001b[1m inference?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The AI tools used in the study on experienced open-source developers are primarily used for code editing </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and natural language processing, whereas the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework models a type of AI that is designed </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">specifically for intent understanding and delegating cryptographically secure inference to specialized models. This</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">implies that the AI tools used in the study are more focused on productivity and coding assistance, whereas the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework is more focused on ensuring data privacy and making privacy-preserving LLM inference </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">practical.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The AI tools used in the study on experienced open-source developers are primarily used for code editing \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand natural language processing, whereas the \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework models a type of AI that is designed \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mspecifically for intent understanding and delegating cryptographically secure inference to specialized models. This\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mimplies that the AI tools used in the study are more focused on productivity and coding assistance, whereas the \u001b[0m\n",
       "\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework is more focused on ensuring data privacy and making privacy-preserving LLM inference \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpractical.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: What are some potential factors that could contribute to the slowing down of experienced open-source </span>\n",
       "<span style=\"font-weight: bold\">developers' productivity when using AI tools, as discovered in a randomized controlled trial in the wild?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: What are some potential factors that could contribute to the slowing down of experienced open-source \u001b[0m\n",
       "\u001b[1mdevelopers' productivity when using AI tools, as discovered in a randomized controlled trial in the wild?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The authors of the study document find evidence that the following factors could contribute to the slowdown</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">effect: tasks defined before randomization (potentially leading to more verbose but functionally equivalent code), </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">prior developer experience with AI tooling (only </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% of developers had prior experience using the Cursor IDE, while</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% had previously used LLMs), and project characteristics such as size and quality standards. Additionally, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">authors suggest that experimental artifacts may not be the primary cause of the slowdown effect, given its </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">robustness across various analyses. However, it's worth noting that these results may not imply that current AI </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">systems are not useful in many realistic, economically relevant settings.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The authors of the study document find evidence that the following factors could contribute to the slowdown\u001b[0m\n",
       "\u001b[1;38;2;118;185;0meffect: tasks defined before randomization \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mpotentially leading to more verbose but functionally equivalent code\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprior developer experience with AI tooling \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0monly \u001b[0m\u001b[1;36m44\u001b[0m\u001b[1;38;2;118;185;0m% of developers had prior experience using the Cursor IDE, while\u001b[0m\n",
       "\u001b[1;36m93\u001b[0m\u001b[1;38;2;118;185;0m% had previously used LLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, and project characteristics such as size and quality standards. Additionally, the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mauthors suggest that experimental artifacts may not be the primary cause of the slowdown effect, given its \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrobustness across various analyses. However, it's worth noting that these results may not imply that current AI \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msystems are not useful in many realistic, economically relevant settings.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How can AI agents be designed to evaluate the outputs of other AI models in a more reliable and </span>\n",
       "<span style=\"font-weight: bold\">cost-effective way, while also promoting trustworthy and scalable evaluation for next-generation language models?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How can AI agents be designed to evaluate the outputs of other AI models in a more reliable and \u001b[0m\n",
       "\u001b[1mcost-effective way, while also promoting trustworthy and scalable evaluation for next-generation language models?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: A new paradigm, called </span><span style=\"color: #008000; text-decoration-color: #008000\">\"agent-as-a-judge,\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> is emerging, which leverages AI agents' reasoning and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">perspective-taking abilities to assess the quality and safety of other models. This approach promises to provide a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">more scalable and nuanced alternative to human evaluation, and research has explored dynamic multi-agent debate </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">frameworks, internal multi-agent debates, and judge module architectures that can continuously evaluate AI outputs </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">in real-time.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: A new paradigm, called \u001b[0m\u001b[32m\"agent-as-a-judge,\"\u001b[0m\u001b[1;38;2;118;185;0m is emerging, which leverages AI agents' reasoning and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mperspective-taking abilities to assess the quality and safety of other models. This approach promises to provide a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmore scalable and nuanced alternative to human evaluation, and research has explored dynamic multi-agent debate \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mframeworks, internal multi-agent debates, and judge module architectures that can continuously evaluate AI outputs \u001b[0m\n",
       "\u001b[1;38;2;118;185;0min real-time.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
    "    )\n",
    "    usr_msg = (\n",
    "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "        f\"Document2: {format_chunk(doc2)}\"\n",
    "    )\n",
    "\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "    pprint2(f\"QA Pair {i+1}\")\n",
    "    pprint2(synth_questions[-1])\n",
    "    pprint(synth_answers[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 4:** Answer The Synthetic Questions\n",
    "\n",
    "In this section, we can implement the third part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- **Use the RAG agent to generate its own answer.**\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"font-weight: bold\">Question: How do the AI tools used in the context of the study on experienced open-source developers (</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Cursor Pro\"</span><span style=\"font-weight: bold\"> </span>\n",
       "<span style=\"font-weight: bold\">and </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Claude 3.5/3.7 Sonnet\"</span><span style=\"font-weight: bold\">) compare to the type of AI modeled in the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"font-weight: bold\"> framework for large language </span>\n",
       "<span style=\"font-weight: bold\">model (LLM) inference?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[1mQuestion: How do the AI tools used in the context of the study on experienced open-source developers \u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"Cursor Pro\"\u001b[0m\u001b[1m \u001b[0m\n",
       "\u001b[1mand \u001b[0m\u001b[32m\"Claude 3.5/3.7 Sonnet\"\u001b[0m\u001b[1m)\u001b[0m\u001b[1m compare to the type of AI modeled in the \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1m framework for large language \u001b[0m\n",
       "\u001b[1mmodel \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLM\u001b[0m\u001b[1m)\u001b[0m\u001b[1m inference?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: So, to answer your question, the AI tools used in the study on experienced open-source developers are </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">primarily </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Cursor Pro\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> and </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Claude 3.5/3.7 Sonnet\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. On the other hand, the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework models a type </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of AI for large language model (LLM) inference.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">According to the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> document, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework is an approach to enabling privileged and protected machine learning, which means it </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">provides a secure way to integrate LLMs into various applications. This framework is designed to allow for more </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">general-purpose LLMs, as opposed to the more specialized AI tools like Cursor Pro and Claude </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.7</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Sonnet used in</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the study.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The study primarily focuses on the impact of these specialized AI tools on developer productivity, whereas the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework is more focused on the underlying mechanics of integrating LLMs. Given that Cursor Pro and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Claude </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.7</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Sonnet are designed to facilitate coding tasks, whereas the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework is a broader </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">framework for integrating LLMs, the two are quite distinct. Therefore, it's challenging to make a direct comparison</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">between them. The study and the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework serve different purposes and cater to different needs.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: So, to answer your question, the AI tools used in the study on experienced open-source developers are \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprimarily \u001b[0m\u001b[32m\"Cursor Pro\"\u001b[0m\u001b[1;38;2;118;185;0m and \u001b[0m\u001b[32m\"Claude 3.5/3.7 Sonnet\"\u001b[0m\u001b[1;38;2;118;185;0m. On the other hand, the \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework models a type \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mof AI for large language model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLM\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m inference.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAccording to the \u001b[0m\u001b[32m\"Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling\"\u001b[0m\u001b[1;38;2;118;185;0m document, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework is an approach to enabling privileged and protected machine learning, which means it \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprovides a secure way to integrate LLMs into various applications. This framework is designed to allow for more \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgeneral-purpose LLMs, as opposed to the more specialized AI tools like Cursor Pro and Claude \u001b[0m\u001b[1;36m3.5\u001b[0m\u001b[1;38;2;118;185;0m/\u001b[0m\u001b[1;36m3.7\u001b[0m\u001b[1;38;2;118;185;0m Sonnet used in\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe study.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe study primarily focuses on the impact of these specialized AI tools on developer productivity, whereas the \u001b[0m\n",
       "\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework is more focused on the underlying mechanics of integrating LLMs. Given that Cursor Pro and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mClaude \u001b[0m\u001b[1;36m3.5\u001b[0m\u001b[1;38;2;118;185;0m/\u001b[0m\u001b[1;36m3.7\u001b[0m\u001b[1;38;2;118;185;0m Sonnet are designed to facilitate coding tasks, whereas the \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework is a broader \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mframework for integrating LLMs, the two are quite distinct. Therefore, it's challenging to make a direct comparison\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbetween them. The study and the \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework serve different purposes and cater to different needs.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">Question: What are some potential factors that could contribute to the slowing down of experienced open-source </span>\n",
       "<span style=\"font-weight: bold\">developers' productivity when using AI tools, as discovered in a randomized controlled trial in the wild?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1mQuestion: What are some potential factors that could contribute to the slowing down of experienced open-source \u001b[0m\n",
       "\u001b[1mdevelopers' productivity when using AI tools, as discovered in a randomized controlled trial in the wild?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: You're referring to the study </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Measuring the Impact of Early-2025 AI on Experienced Open-Source </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Developer Productivity\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. According to the results, several factors could be contributing to the slowdown in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">experienced open-source developers' productivity when using AI tools. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Firstly, it's worth mentioning that the study found that **over-optimism about AI usefulness** played a role. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Despite developers being familiar with the AI tools, they still overestimated the benefits, expecting a </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reduction in implementation time, but actually experiencing a </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% increase in completion time.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Another factor that contributed to the slowdown was **high developer familiarity with repositories**. The study </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">found that developers were working on large, popular repositories they were highly familiar with, which might have </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">affected their productivity.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Additionally, the study proposes several AI-specific factors that could be contributing to the slowdown. These </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">include:</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">*   **Fundamental reliability**: AI systems that are not reliable may slow down developers.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">*   **Latency**: AI systems with high latency may also slow down developers.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">*   **Elicitation**: AI systems that are not well-elicitated (e.g. via skilled prompting or scaffolding) may slow </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">down developers.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">These findings highlight a disconnect between perceived and actual AI impact on developer productivity. Expect </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">experts in economics and machine learning to make different predictions about the effects of AI on productivity.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Finally, the study suggests that **further improvements to current AI systems** (e.g. better prompting, agent </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">scaffolding, or domain-specific fine-tuning) could potentially yield positive speedup in this setting.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: You're referring to the study \u001b[0m\u001b[32m\"Measuring the Impact of Early-2025 AI on Experienced Open-Source \u001b[0m\n",
       "\u001b[32mDeveloper Productivity\"\u001b[0m\u001b[1;38;2;118;185;0m. According to the results, several factors could be contributing to the slowdown in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexperienced open-source developers' productivity when using AI tools. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFirstly, it's worth mentioning that the study found that **over-optimism about AI usefulness** played a role. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mDespite developers being familiar with the AI tools, they still overestimated the benefits, expecting a \u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;38;2;118;185;0m% \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreduction in implementation time, but actually experiencing a \u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;38;2;118;185;0m% increase in completion time.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAnother factor that contributed to the slowdown was **high developer familiarity with repositories**. The study \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfound that developers were working on large, popular repositories they were highly familiar with, which might have \u001b[0m\n",
       "\u001b[1;38;2;118;185;0maffected their productivity.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAdditionally, the study proposes several AI-specific factors that could be contributing to the slowdown. These \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minclude:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m*   **Fundamental reliability**: AI systems that are not reliable may slow down developers.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m*   **Latency**: AI systems with high latency may also slow down developers.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m*   **Elicitation**: AI systems that are not well-elicitated \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0me.g. via skilled prompting or scaffolding\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m may slow \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdown developers.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThese findings highlight a disconnect between perceived and actual AI impact on developer productivity. Expect \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexperts in economics and machine learning to make different predictions about the effects of AI on productivity.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFinally, the study suggests that **further improvements to current AI systems** \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0me.g. better prompting, agent \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mscaffolding, or domain-specific fine-tuning\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m could potentially yield positive speedup in this setting.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"font-weight: bold\">Question: How can AI agents be designed to evaluate the outputs of other AI models in a more reliable and </span>\n",
       "<span style=\"font-weight: bold\">cost-effective way, while also promoting trustworthy and scalable evaluation for next-generation language models?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\u001b[1mQuestion: How can AI agents be designed to evaluate the outputs of other AI models in a more reliable and \u001b[0m\n",
       "\u001b[1mcost-effective way, while also promoting trustworthy and scalable evaluation for next-generation language models?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: That's a great question! To design AI agents that can evaluate the outputs of other AI models more </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reliably and cost-effectively, researchers have proposed a few strategies.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">First, we can build libraries of common evaluation dimensions (e.g., fluency, factuality) and modular persona </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">templates that can be composed for a specific domain [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. This can help AI agents understand what to look for when </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">evaluating outputs. Additionally, multimodal AI models (which can handle images, audio, etc.) are emerging, and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">future agent-judges might need to evaluate outputs that aren't just text, such as image captions or generated </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">graphs [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">].</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Another approach is to develop standard meta-evaluation benchmarks and datasets, which would allow for objective </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">comparison of different evaluation strategies [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. For example, a benchmark could include model outputs for various</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks with careful human scores along multiple axes.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Researchers also suggest designing uncertainty estimates or triggers for fallback to human evaluation, which would </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">make these systems safer in practice [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. This means that AI agents could be designed to recognize when their own </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">evaluation is uncertain or incomplete, and then fall back on human judgment to ensure accuracy.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Finally, exploring user experience and dynamic evaluation could improve the performance of AI agents. For instance,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">an AI tutoring agent might assess its own answer before presenting it to a student, in real-time. This could be </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">done by training a smaller internal critic model that runs alongside the main model, allowing the AI agent to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">evaluate its own outputs on the fly [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">These approaches aim to make AI-based evaluation more reliable, cost-effective, and trustworthy for next-generation</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language models.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">References:</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Yu, F. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: That's a great question! To design AI agents that can evaluate the outputs of other AI models more \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreliably and cost-effectively, researchers have proposed a few strategies.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFirst, we can build libraries of common evaluation dimensions \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0me.g., fluency, factuality\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and modular persona \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtemplates that can be composed for a specific domain \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. This can help AI agents understand what to look for when \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mevaluating outputs. Additionally, multimodal AI models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mwhich can handle images, audio, etc.\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m are emerging, and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfuture agent-judges might need to evaluate outputs that aren't just text, such as image captions or generated \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgraphs \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAnother approach is to develop standard meta-evaluation benchmarks and datasets, which would allow for objective \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcomparison of different evaluation strategies \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. For example, a benchmark could include model outputs for various\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks with careful human scores along multiple axes.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mResearchers also suggest designing uncertainty estimates or triggers for fallback to human evaluation, which would \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmake these systems safer in practice \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. This means that AI agents could be designed to recognize when their own \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mevaluation is uncertain or incomplete, and then fall back on human judgment to ensure accuracy.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFinally, exploring user experience and dynamic evaluation could improve the performance of AI agents. For instance,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0man AI tutoring agent might assess its own answer before presenting it to a student, in real-time. This could be \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdone by training a smaller internal critic model that runs alongside the main model, allowing the AI agent to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mevaluate its own outputs on the fly \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThese approaches aim to make AI-based evaluation more reliable, cost-effective, and trustworthy for next-generation\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage models.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mReferences:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Yu, F. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2025\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Generate some synthetic answers to the questions above.\n",
    "##   Try to use the same syntax as the cell above\n",
    "rag_answers = []\n",
    "for i, q in enumerate(synth_questions):\n",
    "    ## TODO: Compute the RAG Answer\n",
    "    # Extract just the question text (remove \"Question: \" prefix if present)\n",
    "    question_text = q.replace(\"Question: \", \"\") if q.startswith(\"Question: \") else q\n",
    "    \n",
    "    # Use the RAG chain to get the answer\n",
    "    rag_response = \"\"\n",
    "    for token in rag_chain.stream(question_text):\n",
    "        rag_response += token\n",
    "    \n",
    "    rag_answer = rag_response.strip()\n",
    "    rag_answers += [rag_answer]\n",
    "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
    "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 5:** Implement A Human Preference Metric\n",
    "\n",
    "In this section, we can implement the fourth part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- **Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"**\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How do the AI tools used in the context of the study on experienced open-source developers </span>\n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Cursor Pro\"</span><span style=\"font-weight: bold\"> and </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Claude 3.5/3.7 Sonnet\"</span><span style=\"font-weight: bold\">) compare to the type of AI modeled in the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"font-weight: bold\"> framework for </span>\n",
       "<span style=\"font-weight: bold\">large language model (LLM) inference?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How do the AI tools used in the context of the study on experienced open-source developers \u001b[0m\n",
       "\u001b[1m(\u001b[0m\u001b[32m\"Cursor Pro\"\u001b[0m\u001b[1m and \u001b[0m\u001b[32m\"Claude 3.5/3.7 Sonnet\"\u001b[0m\u001b[1m)\u001b[0m\u001b[1m compare to the type of AI modeled in the \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1m framework for \u001b[0m\n",
       "\u001b[1mlarge language model \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLM\u001b[0m\u001b[1m)\u001b[0m\u001b[1m inference?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The AI tools used in the study on experienced open-source developers are primarily used for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">code editing and natural language processing, whereas the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework models a type of AI that is </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">designed specifically for intent understanding and delegating cryptographically secure inference to specialized </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">models. This implies that the AI tools used in the study are more focused on productivity and coding assistance, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">whereas the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework is more focused on ensuring data privacy and making privacy-preserving LLM </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">inference practical.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The AI tools used in the study on experienced open-source developers are primarily used for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcode editing and natural language processing, whereas the \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework models a type of AI that is \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdesigned specifically for intent understanding and delegating cryptographically secure inference to specialized \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodels. This implies that the AI tools used in the study are more focused on productivity and coding assistance, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwhereas the \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework is more focused on ensuring data privacy and making privacy-preserving LLM \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minference practical.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: So, to answer your question, the AI tools used in the study on experienced open-source developers are </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">primarily </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Cursor Pro\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> and </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Claude 3.5/3.7 Sonnet\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. On the other hand, the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework models a type </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of AI for large language model (LLM) inference.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">According to the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> document, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework is an approach to enabling privileged and protected machine learning, which means it </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">provides a secure way to integrate LLMs into various applications. This framework is designed to allow for more </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">general-purpose LLMs, as opposed to the more specialized AI tools like Cursor Pro and Claude </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.7</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Sonnet used in</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the study.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The study primarily focuses on the impact of these specialized AI tools on developer productivity, whereas the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework is more focused on the underlying mechanics of integrating LLMs. Given that Cursor Pro and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Claude </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.7</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Sonnet are designed to facilitate coding tasks, whereas the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework is a broader </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">framework for integrating LLMs, the two are quite distinct. Therefore, it's challenging to make a direct comparison</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">between them. The study and the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agentic-PPML\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> framework serve different purposes and cater to different needs.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: So, to answer your question, the AI tools used in the study on experienced open-source developers are \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprimarily \u001b[0m\u001b[32m\"Cursor Pro\"\u001b[0m\u001b[1;38;2;118;185;0m and \u001b[0m\u001b[32m\"Claude 3.5/3.7 Sonnet\"\u001b[0m\u001b[1;38;2;118;185;0m. On the other hand, the \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework models a type \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mof AI for large language model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLM\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m inference.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAccording to the \u001b[0m\u001b[32m\"Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling\"\u001b[0m\u001b[1;38;2;118;185;0m document, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework is an approach to enabling privileged and protected machine learning, which means it \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprovides a secure way to integrate LLMs into various applications. This framework is designed to allow for more \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgeneral-purpose LLMs, as opposed to the more specialized AI tools like Cursor Pro and Claude \u001b[0m\u001b[1;36m3.5\u001b[0m\u001b[1;38;2;118;185;0m/\u001b[0m\u001b[1;36m3.7\u001b[0m\u001b[1;38;2;118;185;0m Sonnet used in\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe study.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe study primarily focuses on the impact of these specialized AI tools on developer productivity, whereas the \u001b[0m\n",
       "\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework is more focused on the underlying mechanics of integrating LLMs. Given that Cursor Pro and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mClaude \u001b[0m\u001b[1;36m3.5\u001b[0m\u001b[1;38;2;118;185;0m/\u001b[0m\u001b[1;36m3.7\u001b[0m\u001b[1;38;2;118;185;0m Sonnet are designed to facilitate coding tasks, whereas the \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework is a broader \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mframework for integrating LLMs, the two are quite distinct. Therefore, it's challenging to make a direct comparison\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbetween them. The study and the \u001b[0m\u001b[32m\"Agentic-PPML\"\u001b[0m\u001b[1;38;2;118;185;0m framework serve different purposes and cater to different needs.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: What are some potential factors that could contribute to the slowing down of experienced </span>\n",
       "<span style=\"font-weight: bold\">open-source developers' productivity when using AI tools, as discovered in a randomized controlled trial in the </span>\n",
       "<span style=\"font-weight: bold\">wild?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: What are some potential factors that could contribute to the slowing down of experienced \u001b[0m\n",
       "\u001b[1mopen-source developers' productivity when using AI tools, as discovered in a randomized controlled trial in the \u001b[0m\n",
       "\u001b[1mwild?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The authors of the study document find evidence that the following factors could contribute </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to the slowdown effect: tasks defined before randomization (potentially leading to more verbose but functionally </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">equivalent code), prior developer experience with AI tooling (only </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% of developers had prior experience using the</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Cursor IDE, while </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% had previously used LLMs), and project characteristics such as size and quality standards. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Additionally, the authors suggest that experimental artifacts may not be the primary cause of the slowdown effect, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">given its robustness across various analyses. However, it's worth noting that these results may not imply that </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">current AI systems are not useful in many realistic, economically relevant settings.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The authors of the study document find evidence that the following factors could contribute \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mto the slowdown effect: tasks defined before randomization \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mpotentially leading to more verbose but functionally \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mequivalent code\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, prior developer experience with AI tooling \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0monly \u001b[0m\u001b[1;36m44\u001b[0m\u001b[1;38;2;118;185;0m% of developers had prior experience using the\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mCursor IDE, while \u001b[0m\u001b[1;36m93\u001b[0m\u001b[1;38;2;118;185;0m% had previously used LLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, and project characteristics such as size and quality standards. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAdditionally, the authors suggest that experimental artifacts may not be the primary cause of the slowdown effect, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgiven its robustness across various analyses. However, it's worth noting that these results may not imply that \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcurrent AI systems are not useful in many realistic, economically relevant settings.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: You're referring to the study </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Measuring the Impact of Early-2025 AI on Experienced Open-Source </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Developer Productivity\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. According to the results, several factors could be contributing to the slowdown in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">experienced open-source developers' productivity when using AI tools. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Firstly, it's worth mentioning that the study found that **over-optimism about AI usefulness** played a role. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Despite developers being familiar with the AI tools, they still overestimated the benefits, expecting a </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reduction in implementation time, but actually experiencing a </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% increase in completion time.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Another factor that contributed to the slowdown was **high developer familiarity with repositories**. The study </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">found that developers were working on large, popular repositories they were highly familiar with, which might have </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">affected their productivity.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Additionally, the study proposes several AI-specific factors that could be contributing to the slowdown. These </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">include:</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">*   **Fundamental reliability**: AI systems that are not reliable may slow down developers.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">*   **Latency**: AI systems with high latency may also slow down developers.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">*   **Elicitation**: AI systems that are not well-elicitated (e.g. via skilled prompting or scaffolding) may slow </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">down developers.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">These findings highlight a disconnect between perceived and actual AI impact on developer productivity. Expect </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">experts in economics and machine learning to make different predictions about the effects of AI on productivity.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Finally, the study suggests that **further improvements to current AI systems** (e.g. better prompting, agent </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">scaffolding, or domain-specific fine-tuning) could potentially yield positive speedup in this setting.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: You're referring to the study \u001b[0m\u001b[32m\"Measuring the Impact of Early-2025 AI on Experienced Open-Source \u001b[0m\n",
       "\u001b[32mDeveloper Productivity\"\u001b[0m\u001b[1;38;2;118;185;0m. According to the results, several factors could be contributing to the slowdown in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexperienced open-source developers' productivity when using AI tools. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFirstly, it's worth mentioning that the study found that **over-optimism about AI usefulness** played a role. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mDespite developers being familiar with the AI tools, they still overestimated the benefits, expecting a \u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;38;2;118;185;0m% \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreduction in implementation time, but actually experiencing a \u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;38;2;118;185;0m% increase in completion time.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAnother factor that contributed to the slowdown was **high developer familiarity with repositories**. The study \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfound that developers were working on large, popular repositories they were highly familiar with, which might have \u001b[0m\n",
       "\u001b[1;38;2;118;185;0maffected their productivity.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAdditionally, the study proposes several AI-specific factors that could be contributing to the slowdown. These \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minclude:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m*   **Fundamental reliability**: AI systems that are not reliable may slow down developers.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m*   **Latency**: AI systems with high latency may also slow down developers.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m*   **Elicitation**: AI systems that are not well-elicitated \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0me.g. via skilled prompting or scaffolding\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m may slow \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdown developers.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThese findings highlight a disconnect between perceived and actual AI impact on developer productivity. Expect \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexperts in economics and machine learning to make different predictions about the effects of AI on productivity.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFinally, the study suggests that **further improvements to current AI systems** \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0me.g. better prompting, agent \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mscaffolding, or domain-specific fine-tuning\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m could potentially yield positive speedup in this setting.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How can AI agents be designed to evaluate the outputs of other AI models in a more reliable and</span>\n",
       "<span style=\"font-weight: bold\">cost-effective way, while also promoting trustworthy and scalable evaluation for next-generation language models?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How can AI agents be designed to evaluate the outputs of other AI models in a more reliable and\u001b[0m\n",
       "\u001b[1mcost-effective way, while also promoting trustworthy and scalable evaluation for next-generation language models?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: A new paradigm, called </span><span style=\"color: #008000; text-decoration-color: #008000\">\"agent-as-a-judge,\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> is emerging, which leverages AI agents' reasoning </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and perspective-taking abilities to assess the quality and safety of other models. This approach promises to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">provide a more scalable and nuanced alternative to human evaluation, and research has explored dynamic multi-agent </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">debate frameworks, internal multi-agent debates, and judge module architectures that can continuously evaluate AI </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">outputs in real-time.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: A new paradigm, called \u001b[0m\u001b[32m\"agent-as-a-judge,\"\u001b[0m\u001b[1;38;2;118;185;0m is emerging, which leverages AI agents' reasoning \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand perspective-taking abilities to assess the quality and safety of other models. This approach promises to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprovide a more scalable and nuanced alternative to human evaluation, and research has explored dynamic multi-agent \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdebate frameworks, internal multi-agent debates, and judge module architectures that can continuously evaluate AI \u001b[0m\n",
       "\u001b[1;38;2;118;185;0moutputs in real-time.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: That's a great question! To design AI agents that can evaluate the outputs of other AI models more </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reliably and cost-effectively, researchers have proposed a few strategies.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">First, we can build libraries of common evaluation dimensions (e.g., fluency, factuality) and modular persona </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">templates that can be composed for a specific domain [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. This can help AI agents understand what to look for when </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">evaluating outputs. Additionally, multimodal AI models (which can handle images, audio, etc.) are emerging, and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">future agent-judges might need to evaluate outputs that aren't just text, such as image captions or generated </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">graphs [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">].</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Another approach is to develop standard meta-evaluation benchmarks and datasets, which would allow for objective </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">comparison of different evaluation strategies [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. For example, a benchmark could include model outputs for various</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks with careful human scores along multiple axes.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Researchers also suggest designing uncertainty estimates or triggers for fallback to human evaluation, which would </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">make these systems safer in practice [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. This means that AI agents could be designed to recognize when their own </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">evaluation is uncertain or incomplete, and then fall back on human judgment to ensure accuracy.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Finally, exploring user experience and dynamic evaluation could improve the performance of AI agents. For instance,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">an AI tutoring agent might assess its own answer before presenting it to a student, in real-time. This could be </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">done by training a smaller internal critic model that runs alongside the main model, allowing the AI agent to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">evaluate its own outputs on the fly [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">These approaches aim to make AI-based evaluation more reliable, cost-effective, and trustworthy for next-generation</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language models.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">References:</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Yu, F. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: That's a great question! To design AI agents that can evaluate the outputs of other AI models more \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreliably and cost-effectively, researchers have proposed a few strategies.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFirst, we can build libraries of common evaluation dimensions \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0me.g., fluency, factuality\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and modular persona \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtemplates that can be composed for a specific domain \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. This can help AI agents understand what to look for when \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mevaluating outputs. Additionally, multimodal AI models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mwhich can handle images, audio, etc.\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m are emerging, and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfuture agent-judges might need to evaluate outputs that aren't just text, such as image captions or generated \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgraphs \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAnother approach is to develop standard meta-evaluation benchmarks and datasets, which would allow for objective \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcomparison of different evaluation strategies \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. For example, a benchmark could include model outputs for various\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks with careful human scores along multiple axes.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mResearchers also suggest designing uncertainty estimates or triggers for fallback to human evaluation, which would \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmake these systems safer in practice \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. This means that AI agents could be designed to recognize when their own \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mevaluation is uncertain or incomplete, and then fall back on human judgment to ensure accuracy.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFinally, exploring user experience and dynamic evaluation could improve the performance of AI agents. For instance,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0man AI tutoring agent might assess its own answer before presenting it to a student, in real-time. This could be \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdone by training a smaller internal critic model that runs alongside the main model, allowing the AI agent to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mevaluate its own outputs on the fly \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThese approaches aim to make AI-based evaluation more reliable, cost-effective, and trustworthy for next-generation\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage models.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mReferences:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Yu, F. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2025\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Adapt this prompt for whichever LLM you're actually interested in using. \n",
    "## If it's llama, maybe system message would be good?\n",
    "eval_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are an expert evaluator. You must respond with EXACTLY [1] or [2], nothing else in the brackets.\"\n",
    "     \" Be fair but recognize that detailed, well-sourced answers are usually better.\"\n",
    "    ),\n",
    "    (\"user\", \n",
    "     \"Compare these two answers. Assume Answer 1 is acceptable baseline.\\n\\n\"\n",
    "     \"SCORING:\\n\"\n",
    "     \"[1] Answer 2 is worse than Answer 1\\n\" \n",
    "     \"[2] Answer 2 is better than Answer 1\\n\\n\"\n",
    "     \"IMPORTANT: Start your response with [1] or [2] in brackets.\\n\\n\"\n",
    "     \"{qa_trio}\\n\\n\"\n",
    "     \"EVALUATION:\"\n",
    "    )\n",
    "])\n",
    "\n",
    "pref_score = []\n",
    "\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
    "\n",
    "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
    "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
    "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
    "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Congratulations! We now have an LLM system that reasons about our pipeline and tries to evaluate it!** Now that we have some judge results, we can simply aggregate the results and see how often our formulation was according to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
    "print(f\"Preference Score: {pref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Advanced Formulations\n",
    "\n",
    "The exercise above was meant to prepare you for the final assessment of the course and showcased a simple but effective evaluator chain. The objective and implementation details were provided for you, and the logic for using it probably makes sense now that you've seen it in action. \n",
    "\n",
    "With that being said, this metric was merely a product of us specifying:\n",
    "- **What kind of behavior is important for our pipeline to have?**\n",
    "- **What do we need to do in order to exhibit and evaluate this behavior?**\n",
    "\n",
    "From these two questions, we could have come up with plenty of other evaluation metrics that could have assessed different attributes, incorporated different evaluator chain techniques, and even required different pipeline organization strategies. Though far from an exhaustive list, some common formulations you will likely come across may include:\n",
    "\n",
    "- **Style Evaluation:** Some evaluation formulations can be as simple as \"let me ask some questions and see if the output feels desirable.\" This might be used to see whether a chatbot \"acts like it's supposed to\" based on a description provided to a judge LLM. We're using quotations since this kind of assessment can reasonably be achieved with nothing but prompt engineering and a while loop.\n",
    "\n",
    "- **Ground-Truth Evaluation:** In our chain, we used synthetic generation to create some random questions and answers using a sampling strategy, but in reality you may actually have some representative questions and answers that you need your chatbot to consistently get right! In this case, a modification of the exercise chain above should be implemented and closely monitored as you develop your pipelines.\n",
    "\n",
    "- **Retrieval/Augmentation Evaluation:** This course made many assumptions about what kinds of preprocessing and prompting steps would be good for your pipelines, and much of this was determined by experimentation. Factors such as document preprocessing, chunking strategies, model selection, and prompt specification all played important roles, so creating metrics to validate these decisions may be of interest. This kind of metric might require your pipeline to output your context chunks or may even rely solely on embedding similarity comparisons, so keep this in mind when trying to implement a chain that works with multiple evaluation strategies. Consider the [**RagasEvaluatorChain**](https://docs.ragas.io/en/stable/howtos/integrations/langchain.html) abstraction as a decent starting point for making an custom generalizable evaluation routine. \n",
    "\n",
    "- **Trajectory Evaluation:** Using more advanced agent formulations, you can implement multiple-query strategies that assume the presence of conversational memory. With this, you can implement an evaluation agent which can:\n",
    "    - Ask a series of questions in order to evaluate how well the agent is able to adapt and cater to the scenario. This kind of system generally considers a series of correspondence and aims to tease out and evaluate a \"trajectory\" of how the agent navigated the conversation. The [**LangChain Trajectory Evaluators documentation**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/trajectory/) is a good starting point.\n",
    "    - Alternatively, you could also implement an evaluation agent that tries to achieve objectives by interacting with the chatbot. Such an agent can output whether they were able to navigate to their solution in a natural manner, and can even be used to generate a report about the percieved performance. The [**LangChain Agents documentation**](https://python.langchain.com/v0.1/docs/modules/agents/) is a good starting point!\n",
    "\n",
    "<br>\n",
    "\n",
    "At the end of the day, just make sure to use the tools you have at your disposal appropriately. By this point in the course, you should already be well-acquainted with the LLM core value propositions: **They're powerful, scalable, predictable, controllable, and orchestratable... but will act unpredictably when you just expect them to work by default.** Assess your needs, formulate and validate your pipelines, give enough information, and add as much control as you can to make your system work consistently, efficiently, and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5: [Assessment]** Evaluating For Credit\n",
    "\n",
    "Welcome to the last exercise of the course! Hopefully you've enjoyed the material and are ready to actually get credit for these notebooks! For this part:\n",
    "\n",
    "- **Make sure you're in the course environment**\n",
    "- **Make sure `docstore_index/` has been uploaded to the course environment...**\n",
    "    - **...and contains [at least one Arxiv paper](https://arxiv.org/search/advanced) which has been updated recently.**\n",
    "- **Make sure you don't have some old session of [`09_langserve.ipynb`](09_langserve.ipynb) already occupying the port. Your assessment requires you to implement the new `/retriever` and `/generator` endpoints!!**\n",
    "\n",
    "**Objective:** On launch, [**`frontend/frontend_block.py`**](frontend/frontend_block.py) had several lines of code which trigger the course pass condition. Your objective is to invoke that series of commands by using your pipeline to pass the **Evaluation** check! Recall [`09_langserve.ipynb`](09_langserve.ipynb) and use it as a starting example! As a recommendation, consider duplicating it so that you can keep the original as an authoritative reference. \n",
    "\n",
    "**Once Finished:** While your course environment is still open, please navigate back to your course environment launcher area and click the **\"Assess Task\"** button! After that, you're all done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48e300ed-951c-4006-ac54-cbbd41251707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = 'http://'+window.location.host+':8090';\n",
       "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**Congratulations On Completing The Course**</font>\n",
    "\n",
    "Hopefully this course was not only exciting and challenging, but also adequately prepared you for work on the cutting edge of LLM and RAG system development! Going forward, you should have the skills necessary to tackle industry-level challenges and explore RAG deployment with open-source models and frameworks.\n",
    "\n",
    "**Some NVIDIA-specific releases related to this that you may find interesting include:**\n",
    "- [**NVIDIA NIM**](https://www.nvidia.com/en-us/ai/), which offers microservice spinup routines that can be deployed on local compute.\n",
    "- [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) is the current recommended framework for deploying GPU-accelerated LLM model engines in production settings.\n",
    "- [**NVIDIA's Generative AI Examples Repo**](https://github.com/NVIDIA/GenerativeAIExamples), which includes the current canonical microservice example application and will be updated with new resources as new production workflows get released.\n",
    "- [**The Knowledge-Based Chatbot Technical Brief**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief) which discusses additional publicly-accessible details on productionalizing RAG systems.\n",
    "\n",
    "**Additionally, some key topics you may be interested in delving more into include:**\n",
    "- [**LlamaIndex**](https://www.llamaindex.ai/), which has strong components that can augment and occasionally improve upon the LangChain RAG features.\n",
    "- [**LangSmith**](https://docs.smith.langchain.com/), an upcoming agent productionalization service offered by LangChain.\n",
    "- [**Gradio**](https://www.gradio.app/), though touched on in the course, has many more interface options which will be worth investigating. For inspiration, consider checking out [**HuggingFace Spaces**](https://huggingface.co/spaces) for examples.\n",
    "- [**LangGraph**](https://python.langchain.com/docs/langgraph/) is a framework for graph-based LLM orchestration, and is a natural next step forward for those interested in [multi-agent workflows](https://blog.langchain.dev/langgraph-multi-agent-workflows/).\n",
    "- [**DSPy**](https://github.com/stanfordnlp/dspy), a flow engineering framework that allows you to optimize LLM orchestration pipelines based on empirical performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035451c9-ed12-4bc3-b468-04db5c399e03",
   "metadata": {
    "id": "035451c9-ed12-4bc3-b468-04db5c399e03"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
