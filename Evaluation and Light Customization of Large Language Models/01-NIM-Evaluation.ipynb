{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6296d2",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30afe4d4",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\">**Notebook 1:** Evaluation of a deployed NIM</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a43b9a",
   "metadata": {},
   "source": [
    "**Welcome to the introductory notebook on evaluating NVIDIA Inference Microservices (NIMs).** In this notebook, you'll explore several techniques for assessing large language models (LLMs). We've deployed a NIM instance running the Llama 3.2 3B instruct model, which you'll query as you test and compare various evaluation methods.\n",
    "\n",
    "Objectives:\n",
    "- Learn to query NIMs using both curl and Python.\n",
    "- Understand benchmark evaluation by testing on the MMLU task.\n",
    "- Conduct an LLM-as-a-judge evaluation.\n",
    "- Discover the fundamentals of human evaluation and explore the application of ELO ranking within the LLM arena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a67aa",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Notebook Presentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7752b-6304-4e57-8955-082183c7f3f4",
   "metadata": {},
   "source": [
    "Run the following cell to load a video presentation covering this notebook's topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "978b73e0-83e4-46eb-b8d2-bbf1e96220c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video controls width=\"640\" height=\"360\">\n",
       "        <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-34-v1/notebook-1.mp4\" type=\"video/mp4\">\n",
       "        Your browser does not support the video tag.\n",
       "    </video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from videos.walkthroughs import notebook_01_video as video\n",
    "video()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6bb5fb-9602-4d98-a41d-ad1a3652519b",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Getting Started With Evaluating NIM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da23f6-c9e2-4f1d-8ace-59ee2d03af99",
   "metadata": {},
   "source": [
    "To get started, let's verify that the NIM is operational by checking its health endpoint. You can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef58cd8a-38a8-4bef-a699-56a228deb787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service is up and running!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from IPython.display import clear_output\n",
    "\n",
    "url = \"http://llama3-2-3b-instruct.local/v1/health/ready\"\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Service is up and running!\")\n",
    "            break\n",
    "        else:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Waiting for service to be ready... (Status code:\", response.status_code,\")\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Waiting for service to be ready... (Error:\", e,\")\")\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b75eb24-a6e8-444b-8d50-d54e3a888542",
   "metadata": {},
   "source": [
    "After executing the request, you should see a message stating \"Service is up and running\" Next, verify the model deployed on the NIM by checking its model endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872fe8f9-165e-4632-8d6d-2a11348c8c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta/llama-3.2-3b-instruct\n"
     ]
    }
   ],
   "source": [
    "!curl -s -X GET 'llama3-2-3b-instruct.local/v1/models' | jq -r '.data[0].id'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a729a-1ad3-4627-a03f-312ca867b6cb",
   "metadata": {},
   "source": [
    "The model should be `meta/llama-3.2-3b-instruct`. \n",
    "A common first step in evaluating a model is to \"eyeball\" its responses—that is, to visually inspect the output for a known query. For example, you can use curl to ask the LLM for the capital of Spain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43bce490-ddd8-4765-b750-17bfaf2fe20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Spain is Madrid.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -s -X 'POST' \\\n",
    "'http://llama3-2-3b-instruct.local/v1/chat/completions' \\\n",
    "   -H 'accept: application/json' \\\n",
    "   -H 'Content-Type: application/json' \\\n",
    "   -d '{\n",
    "      \"model\": \"meta/llama-3.2-3b-instruct\",\n",
    "      \"messages\": [\n",
    "          {\n",
    "              \"role\": \"system\",\n",
    "              \"content\": \"Be succint in your response.\"\n",
    "          },\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"What is the capital of Spain?\"\n",
    "          }\n",
    "        ],\n",
    "      \"max_tokens\": 32\n",
    "    }' | jq -r '.choices[0].message.content'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c4755-df4a-48da-bfd5-88362a076430",
   "metadata": {},
   "source": [
    "Assuming the response is \"Madrid\" — Spain's capital — you can then experiment with more complex queries.\n",
    "\n",
    "While eyeballing responses is a useful initial check, this method has several limitations:\n",
    "- It is slow and doesn't scale well, as each question must be processed individually.\n",
    "- It focuses on specific, subject-dependent queries.\n",
    "- It doesn't provide a quantitative score for comparing different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf65a8-30fb-4c95-98e7-97ad69f1471f",
   "metadata": {},
   "source": [
    "Another evaluation approach involves analyzing the response for specific keywords. Metrics like BLEU and ROUGE, developed over 20 years ago for early machine translation systems, follow this strategy. This method enables fast evaluation using parsing libraries, without relying on LLMs or human reviewers.\n",
    "\n",
    "For example, let's define a question and its ground truth answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5fb9cc-0499-41f8-a4d5-b04ed11eaedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the capital of Spain?\"\n",
    "ground_truth = \"Madrid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82180f-b4a4-41bb-8bf4-7d8db0d7f98e",
   "metadata": {},
   "source": [
    "Now, let's query the LLM and analyze its response to check for the presence of the ground truth answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6501e1c-afc3-4454-8a0f-1d846d21bfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated answer contains the ground truth Madrid\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://llama3-2-3b-instruct.local/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"meta/llama-3.2-3b-instruct\",\n",
    "    \"max_tokens\": 300,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"Be succint in your response.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"}\n",
    "    ],\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    if ground_truth in result[\"choices\"][0][\"message\"][\"content\"]:\n",
    "        print(f\"The generated answer contains the ground truth {ground_truth}\")\n",
    "    else:\n",
    "        print(f\"The generated answer does not contain the ground truth {ground_truth}\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2cfd6f-edb1-45f2-95ef-536394c14e25",
   "metadata": {},
   "source": [
    "Parsing for specific words is still not a systematic approach to evaluating LLMs, as two sentences can have the same meaning while using entirely different words.\n",
    "\n",
    "In the remainder of this notebook, you will explore three widely used evaluation methods for LLMs:\n",
    "\n",
    "- Benchmark evaluation – Measuring performance using standardized datasets.\n",
    "- LLM-as-a-judge evaluation – Leveraging another LLM to assess responses.\n",
    "- Human evaluation – Incorporating human judgment, including the ELO ranking system for model comparison in the LLM arena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe33551-a0af-4e7f-af39-36c03ed4b68a",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Benchmark Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3f43e-37ca-4ebd-a7c1-e32c323b27c9",
   "metadata": {},
   "source": [
    "LLMs can be evaluated using benchmarks — carefully curated tasks that assess capabilities in areas such as commonsense reasoning, question answering, and summarization. These evaluations typically involve either parsing the generated response or analyzing the model’s log probabilities (logprobs) for a given input. Most state-of-the-art LLMs are released with a comprehensive set of benchmark evaluations to showcase their performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b43de783-63e6-49e6-b9c6-2bd38c8e2fbf",
   "metadata": {},
   "source": [
    "A widely-used benchmark for evaluating reasoning in large language models is [GSM8K](https://huggingface.co/datasets/openai/gsm8k). GSM8K (Grade-School Math 8K) contains 8.5k open-ended, grade-school math word problems. Each problem requires multi-step quantitative reasoning and expects a short free-form numerical answer rather than a multiple-choice selection.\n",
    "\n",
    "Example problem\n",
    "```\n",
    "A fruit seller packs apples equally into 4 boxes.  \n",
    "If she puts 18 apples in each box and still has 12 apples left over,  \n",
    "how many apples did she have in total?\n",
    "```\n",
    "\n",
    "Solution sketch  \n",
    "• Apples in the packed boxes: $$4 \\times 18 = 72$$  \n",
    "• Add the leftovers: $$72 + 12 = 84$$  \n",
    "\n",
    "Final answer: 84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78ec4f-b57d-4c7a-bcb0-47e65c5b4725",
   "metadata": {},
   "source": [
    "Let's evaluate the LLM of our NIM with the GSM8K benchmark on 10 problem (you can modify the limit value). You can leverage the open-source library [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "850050fe-1e3a-4285-a773-70b30e0a0de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06:10:36:27 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2025-08-06:10:36:27 INFO     [__main__:440] Selected Tasks: ['gsm8k']\n",
      "evaluator:163] Model appears to be an instruct variant but chat template is not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).\n",
      "2025-08-06:10:36:27 INFO     [evaluator:189] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-06:10:36:27 INFO     [evaluator:227] Initializing local-completions model, with arguments: {'model': 'meta/llama-3.2-3b-instruct', 'base_url': 'http://llama3-2-3b-instruct.local/v1/completions', 'tokenizer': 'utils/llama3-1-8b-instruct-tokenizer'}\n",
      "els:168] Using max length 2048 - 1ls.api_mod\n",
      "equests are disabled. To enable concurrent requests, set `num_concurrent` > 1.\n",
      "NFO     [models.api_models:187] Using tokenizer huggingface\n",
      "ples/s]ing train split: 100%|██████████| 7473/7473 [00:00<00:00, 636827.93 exam\n",
      "les/s]ting test split: 100%|██████████| 1319/1319 [00:00<00:00, 463767.87 examp\n",
      "2025-08-06:10:36:31 INFO     [evaluator:290] gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}\n",
      "2025-08-06:10:36:31 INFO     [api.task:434] Building contexts for gsm8k on rank 0...\n",
      "100%|██████████| 10/10 [00:00<00:00, 418.47it/s]\n",
      "2025-08-06:10:36:31 INFO     [evaluator:559] Running generate_until requests\n",
      "Requesting API: 100%|██████████| 10/10 [00:08<00:00,  1.19it/s]\n",
      "2025-08-06:10:36:40 INFO     [loggers.evaluation_tracker:209] Saving results aggregated\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "lm_eval --model local-completions \\\n",
    "        --tasks gsm8k \\\n",
    "        --output_path llama32_gsm8k \\\n",
    "        --model_args model=meta/llama-3.2-3b-instruct,base_url=http://llama3-2-3b-instruct.local/v1/completions,tokenizer=utils/llama3-1-8b-instruct-tokenizer \\\n",
    "        --limit 10 |  awk -F'|' '/acc/ {gsub(/^ +| +$/, \"\", $8); print \"The score is\", $8}' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88455df5-c47c-48a1-80ce-45658616ea50",
   "metadata": {},
   "source": [
    "The score should be 0.6-0.8, which means that 6-8 out of the 10 questions were answered correctly by the LLM. Run the following cell to print the results - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f56e543d-7cb5-481f-875d-0ed86cd112ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"alias\": \"gsm8k\",\n",
      "  \"exact_match,strict-match\": 0.4,\n",
      "  \"exact_match_stderr,strict-match\": 0.16329931618554522,\n",
      "  \"exact_match,flexible-extract\": 0.4,\n",
      "  \"exact_match_stderr,flexible-extract\": 0.16329931618554522\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "dir_path = \"llama32_gsm8k/meta__llama-3.2-3b-instruct\"\n",
    "# Find the latest file starting with \"results\"\n",
    "files = [f for f in os.listdir(dir_path) if f.startswith(\"results\")]\n",
    "if not files:\n",
    "    print(\"No results file found.\")\n",
    "    exit(1)\n",
    "latest_file = max(files, key=lambda f: os.path.getmtime(os.path.join(dir_path, f)))\n",
    "full_path = os.path.join(dir_path, latest_file)\n",
    "\n",
    "# Load JSON and print only results[\"gsm8k\"]\n",
    "with open(full_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    gsm8k_results = data.get(\"results\", {}).get(\"gsm8k\")\n",
    "    print(json.dumps(gsm8k_results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044cfa89-18b1-4da6-850d-fd00e4d77f75",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **LLM-as-a-judge Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19648684-2457-4618-b29a-c9b8a7182569",
   "metadata": {},
   "source": [
    "Benchmark evaluation works pretty well when there is a pre-compiled list of questions with defined answers. However, for many real-life scenarios, there are other factors to consider, such as factual accuracy, style or correctness. LLM-as-a-judge provides an intermediate approach between automatic benchmark evaluations and human evaluations, based on using a second LLM to assess the generated answer of the first LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88f80c-d4cf-4820-bebb-5241df542d7b",
   "metadata": {},
   "source": [
    "For LLM-as-a-judge evaluation, two key components are required in addition to the LLM itself:\n",
    "\n",
    "- An evaluation dataset – A set of questions with acceptable answers, ideally curated by human annotators with subject matter expertise (potentially assisted by LLMs).\n",
    "- A well-crafted prompt – Clear instructions guiding the LLM on how to assess responses.\n",
    "\n",
    "For example, here’s a prompt designed to evaluate faithfulness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636c3f62-be6b-4994-8141-0f07d3b1af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_faithfulness = \"\"\"\n",
    "System: You are an impartial judge assessing the faithfulness of an AI-generated response. Faithfulness means the response accurately reflects the ground-truth response without adding false or misleading content.\n",
    "\n",
    "Evaluation Criteria:\n",
    "1. **Correctness**: Does the response correctly reflect facts from the ground truth?\n",
    "2. **No Hallucination**: Does the response introduce any information not present in the ground truth?\n",
    "3. **Completeness**: Does the response omit any key facts that would change the meaning?\n",
    "4. **Paraphrase Accuracy**: If paraphrased, is the meaning preserved?\n",
    "\n",
    "Your Task:\n",
    "Provide a faithfulness score from **1 (poor)** to **5 (perfect)** and a brief explanation of your reasoning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2f5c0-ed99-4ba6-b198-58edadffdb4d",
   "metadata": {},
   "source": [
    " We recommend that you check out other examples of prompts for LLM-as-a-judge evaluation: see  [the Ragas prompt for faithfulness](https://github.com/explodinggradients/ragas/blob/main/docs/concepts/metrics/available_metrics/faithfulness.md), [the hallucination detection prompt in Opik](https://www.comet.com/docs/opik/evaluation/metrics/hallucination/?from=llm&utm_source=opik&utm_medium=github&utm_content=hallucination_link&utm_campaign=opik#hallucination-prompt), or [the tone evaluation prompt in Promptfoo](https://github.com/promptfoo/promptfoo/blob/7c1576bf01579af23b0d0377df017fe715e1a622/site/docs/guides/langchain-prompttemplate.md?plain=1#L10). Other popular categories are factuality or correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d13ef-525a-4a69-a345-572f38ebf034",
   "metadata": {},
   "source": [
    "For example, let's assume we use our NIM with `llama-3.2-3b-instruct` as the LLM-as-a-judge. We'll ask it to identify the three most populous countries worldwide. The ground truth reflects accurate data as of February 2025, but in this simulated example, the generated answer contains an error in one of the three countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f75bd712-f723-40c9-859a-577b0b47a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_countries = \"What are the three most populated countries worldwide?\"\n",
    "ground_truth_countries = \"India, China and the United States\"\n",
    "generated_answer_countries = \"India, China and Spain\" # Note that Spain is not one of the three most populated countries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa7460-5ae0-48df-886f-14eafbefadca",
   "metadata": {},
   "source": [
    "Let's use the NIM as the LLM-as-a-judge with the faithfulness prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3593366c-0018-43af-80cb-6a497eba0cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Faithfulness Score: 1 (poor)\n",
      "\n",
      "Explanation: The response is incorrect in two significant ways. Firstly, it introduces a new country, Spain, which is not among the top three most populated countries in the world. Secondly, it omits two of the correct countries, the United States and India, which are indeed among the most populated countries globally. This response demonstrates a lack of correctness and introduces false information, making it a poor faithfulness score.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://llama3-2-3b-instruct.local/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"meta/llama-3.2-3b-instruct\",\n",
    "    \"max_tokens\": 300,\n",
    "    \"temperature\": 0.2,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": f\"{prompt_faithfulness}. Question: {question_countries}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Ground truth: {ground_truth_countries}. Answer: {generated_answer_countries}\"}\n",
    "    ],\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Assistant:\", result[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b58955-17f9-4570-8910-3ddaa5fb069d",
   "metadata": {},
   "source": [
    "The LLM-as-a-judge should assign a low faithfulness score of 1, as one of the most populous countries was incorrect. While we've reduced the temperature to make the scoring more deterministic, the score may still vary occasionally.\n",
    "\n",
    "By leveraging LLM-as-a-judge, you can generate numerical scores to evaluate key properties of the model, such as faithfulness. These scores provide a quantifiable way to compare different models systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf3ebe-895e-4719-b6f3-fbc49bb2e56c",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Optional - Human Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e2828-452d-439e-a2c1-c546c2b67f61",
   "metadata": {},
   "source": [
    "LLM-as-a-judge is a powerful evaluation technique, but it comes with potential biases. For example, it may favor responses generated by models similar to itself or show inconsistencies in assigning numerical scores. For more details, see [this blogpost](https://huggingface.co/blog/clefourrier/llm-evaluation).\n",
    "\n",
    "In general, we recommend using LLM-as-a-judge for an initial evaluation pass. However, it is crucial to incorporate human evaluation in a second stage before deploying to production. This ensures that any issues not identified by the LLM-as-a-judge are caught.\n",
    "\n",
    "The first step in human evaluation involves annotating a set of questions and reference answers. These questions are then passed to the LLM to generate answers, and humans are tasked with comparing the reference answers to the generated ones.\n",
    "\n",
    "Our recommendations for a systematic human evaluation process are:\n",
    "- Provide well-curated reference answers.\n",
    "- Instruct humans to compare the reference and generated answers across clear, measurable categories.\n",
    "- Ask evaluators to assign binary scores per category (e.g., 1 for \"yes\" and 0 for \"no\").\n",
    "- Include multiple evaluators for the same question to reduce bias.\n",
    "- Be mindful of human fatigue to maintain consistency in evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f2ec0-9705-45c3-bfb1-e29c0234186e",
   "metadata": {},
   "source": [
    "Let's apply human evaluation to an example involving a chatbot designed for customer support in a telecommunications company. The chatbot is tasked with answering questions about a customer's previous and current bills, with access to the bills through a RAG (Retrieval Augmented Generation) system. The following are the customer query, reference answer, and generated answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2bd4ebf-e306-4925-9c27-74e99b3a2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi, I would like to understand why my phone bill is higher this month compared to last month?\"\n",
    "context = \"...\" # The context contains information of the previous and current bill\n",
    "\n",
    "# The ground truth is provided by subject matter experts\n",
    "reference_answer = \"\"\"\n",
    "    Hello, thanks for reaching out. Let me help you. \n",
    "    Your bill this month is $10 higher because you bought a package of 5GB of data on the 1st of February 2025.\n",
    "\"\"\"\n",
    "# Generated answer by LLM with question and context\n",
    "generated_answer = \"\"\"\n",
    "    Hi, your bill is $10 higher because there is a charge of 5GB of data. There is another charge of $20 for calls.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d88eb1-1c61-4fd6-ba2b-4646bb21caef",
   "metadata": {},
   "source": [
    "For the human evaluation, we ask the evaluators to provide binary scores in the following categories:\n",
    "\n",
    "1) Friendliness: Does the generated answer contain a friendly greeting?\n",
    "- Human assesment: No, it just replies \"Hi.\"\n",
    "- Score: 0\n",
    "2) Correctness: Does the generated answer correctly explain the reason for the issue?\n",
    "- Human assessment: Yes, it correctly states that the charge is due to data usage.\n",
    "- Score: 1\n",
    "\n",
    "3) Relevancy: Does the generated answer contain only relevant information and no hallucinated facts?\n",
    "- Human assessment: No, it includes an irrelevant detail about call charges.\n",
    "- Score: 0\n",
    "\n",
    "**Average human score: 0.33**\n",
    "\n",
    "\n",
    "These human scores can be improved with further customization of the LLM. In some cases, simple prompt engineering with tailored instructions may be sufficient. For more advanced customization, we recommend using the NeMo framework to apply techniques such as LoRA (Low-Rank Adaptation) or supervised fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a25e0-dc32-40a2-b31b-99ba2af92a1c",
   "metadata": {},
   "source": [
    "#### **Human evaluation based on ELO for LLM Arena**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807080d3-5df9-48ba-9848-b25121f5b3e1",
   "metadata": {},
   "source": [
    "A specific variant of human evaluation is ELO ranking, which is used to compare LLMs head-to-head. Originally developed for chess, ELO ranking has now become a widely adopted method for ranking LLMs. For example, the [Chatbot Arena](https://lmarena.ai/?gad_source=1&gclid=EAIaIQobChMIv4rPwNTxiwMVkc_CBB2XNy-PEAAYASAAEgJgJPD_BwE) uses the ELO ranking system for this purpose.\n",
    "\n",
    "In the context of LLMs, a human evaluator compares two responses generated by different models in response to the same prompt. The evaluator simply chooses their preferred response: the winning model gains points, and the losing model loses points in the ELO ranking system. For more information, check out this [blog post](https://huggingface.co/blog/clefourrier/llm-evaluation).\n",
    "\n",
    "As an example, the following code simulates an ELO ranking system for popular LLMs, using random probabilities (which don't represent the actual quality of the models). The code simulates 20 \"battles\" and outputs the final ELO score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec758af5-9139-4930-9918-2e46db7fec44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Match: Gemini vs GPT-4\n",
      "🏆 Winner: Gemini\n",
      "🔹 Match: GPT-4 vs Llama-3\n",
      "🏆 Winner: Llama-3\n",
      "🔹 Match: Llama-3 vs GPT-4\n",
      "🏆 Winner: GPT-4\n",
      "🔹 Match: Claude-3 vs GPT-4\n",
      "🏆 Winner: GPT-4\n",
      "🔹 Match: GPT-4 vs Llama-3\n",
      "🏆 Winner: GPT-4\n",
      "🔹 Match: Gemini vs GPT-4\n",
      "🏆 Winner: Gemini\n",
      "🔹 Match: Claude-3 vs Gemini\n",
      "🏆 Winner: Claude-3\n",
      "🔹 Match: Claude-3 vs Gemini\n",
      "🏆 Winner: Claude-3\n",
      "🔹 Match: Claude-3 vs Llama-3\n",
      "🏆 Winner: Llama-3\n",
      "🔹 Match: Claude-3 vs GPT-4\n",
      "🏆 Winner: GPT-4\n",
      "🔹 Match: Llama-3 vs Gemini\n",
      "🏆 Winner: Llama-3\n",
      "🔹 Match: GPT-4 vs Llama-3\n",
      "🏆 Winner: GPT-4\n",
      "🔹 Match: Claude-3 vs Gemini\n",
      "🏆 Winner: Gemini\n",
      "🔹 Match: GPT-4 vs Claude-3\n",
      "🏆 Winner: GPT-4\n",
      "🔹 Match: Claude-3 vs Gemini\n",
      "🏆 Winner: Claude-3\n",
      "🔹 Match: Gemini vs Claude-3\n",
      "🏆 Winner: Gemini\n",
      "🔹 Match: Gemini vs GPT-4\n",
      "🏆 Winner: Gemini\n",
      "🔹 Match: GPT-4 vs Claude-3\n",
      "🏆 Winner: Claude-3\n",
      "🔹 Match: GPT-4 vs Claude-3\n",
      "🏆 Winner: GPT-4\n",
      "🔹 Match: Llama-3 vs Claude-3\n",
      "🏆 Winner: Llama-3\n",
      "\n",
      "📊 Final ELO Ratings:\n",
      "GPT-4: 1526\n",
      "Gemini: 1513\n",
      "Llama-3: 1513\n",
      "Claude-3: 1448\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "# Initialize chatbot ratings\n",
    "chatbots = {\n",
    "    \"GPT-4\": 1500,\n",
    "    \"Claude-3\": 1500,\n",
    "    \"Gemini\": 1500,\n",
    "    \"Llama-3\": 1500\n",
    "}\n",
    "\n",
    "def expected_score(rating_a, rating_b):\n",
    "    \"\"\"Calculate the expected score for Chatbot A vs Chatbot B.\"\"\"\n",
    "    return 1 / (1 + math.pow(10, (rating_b - rating_a) / 400))\n",
    "\n",
    "def update_elo(winner, loser, k=32):\n",
    "    \"\"\"Update the ELO ratings after a match.\"\"\"\n",
    "    expected_winner = expected_score(chatbots[winner], chatbots[loser])\n",
    "    expected_loser = expected_score(chatbots[loser], chatbots[winner])\n",
    "\n",
    "    chatbots[winner] += round(k * (1 - expected_winner))\n",
    "    chatbots[loser] += round(k * (0 - expected_loser))\n",
    "\n",
    "def simulate_battle():\n",
    "    \"\"\"Simulate a random chatbot battle with a human-like decision.\"\"\"\n",
    "    bot1, bot2 = random.sample(list(chatbots.keys()), 2)\n",
    "    print(f\"🔹 Match: {bot1} vs {bot2}\")\n",
    "\n",
    "    # Simulate a winner (you can replace this with a real evaluation)\n",
    "    winner = bot1 if random.random() < 0.55 else bot2  # Small skill bias\n",
    "    loser = bot2 if winner == bot1 else bot1\n",
    "\n",
    "    print(f\"🏆 Winner: {winner}\")\n",
    "    update_elo(winner, loser)\n",
    "\n",
    "# Simulate 20 battles\n",
    "for _ in range(20):\n",
    "    simulate_battle()\n",
    "\n",
    "# Show final rankings\n",
    "sorted_bots = sorted(chatbots.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n📊 Final ELO Ratings:\")\n",
    "for bot, rating in sorted_bots:\n",
    "    print(f\"{bot}: {rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9b15c-552c-48c6-ac74-e6f1daee4020",
   "metadata": {},
   "source": [
    "Please note that the scores in this simulation are not indicative of the actual quality of the models.\n",
    "\n",
    "After completing this notebook, proceed to the next one, where you'll learn how the NeMo Evaluator Microservice simplifies the process of evaluating LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa6cf0-7f17-4d8f-8ebb-9785a1ac8a4a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
